{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM UOC SALINIDAD EN EL VALLE DEL GUADALHORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan las librerias que serán necesarias durante todo el desarrollo del script\n",
    "import matplotlib as mpl\n",
    "import numpy\n",
    "from matplotlib.colors import ListedColormap\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from datetime import datetime\n",
    "import pydotplus\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga e integración de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso que se deberán cargar e integrar los datos, ya que se encuentran dispersos en distintos ficheros.\n",
    "\n",
    "A continuación se ejecuta un código que va recorriendo cada fichero CSV y va extrayendo la información de cada variable. Se crea una variable fecha, con el día, mes y año donde se ha tomada cada dato. \n",
    "\n",
    "Por lo tanto, crearemos un dataframe con todos los datos recogidos por la Junta de Andalucía en la explotación del sistema de presas del valle del guadalhorce. \n",
    "\n",
    "Adicionalmente se visualiza el dataset completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se leen los ficheros csv correspondientes.\n",
    "\n",
    "df_list=[]\n",
    "for anyo in range(12,20):\n",
    "    path_csv=f'dataset/ESTADILLO_DE_EMBALSES_20{anyo}.csv'\n",
    "    df=pd.read_csv(path_csv,sep=\";\",encoding = 'unicode_escape',decimal=',',thousands='.')\n",
    "    #Se consolidan las variables Dia, Mes y Anyo en una variable de nueva creación llamada Fecha\n",
    "    df['aux']=df['Anyo'].astype(str) + df['Mes'].astype(str).str.zfill(2)+ df['Dia'].astype(str).str.zfill(2)\n",
    "    df['Fecha'] = pd.to_datetime(df['aux'], format='%Y%m%d')\n",
    "    #Eliminamos la columna auxilir 'aux'\n",
    "    del df['aux']\n",
    "    #Se cambia el formato de YYYY-MM-DD a DD-MM-YYYY\n",
    "    df['Fecha'] = df['Fecha'].dt.strftime('%d-%m-%Y')\n",
    "    #Movemos el campo fecha al comienzo del dataframe\n",
    "    indice=df['Fecha']\n",
    "    df.drop(labels=['Fecha'], axis=1,inplace = True)\n",
    "    df.insert(0, 'Fecha', indice)\n",
    "    if anyo==14:\n",
    "        df.drop(df.columns[[74,75,76,77,78,79,80,81,82,83,84,85,86]],axis = 1, inplace = True)\n",
    "    if anyo==15:\n",
    "        df.drop(df.columns[[74,75,76,77,78,79,80,81,82,83,84,85,86]],axis = 1, inplace = True)\n",
    "    df_list.append(df)\n",
    "\n",
    "df_data=pd.concat(df_list,ignore_index = True)\n",
    "#Se consolidan todos los dataframes de la lista en un solo dataframe. Así, quedará un dataframe que contenga los\n",
    "#datos díarios dentro de un año.\n",
    "    \n",
    "#Se utiliza para visualizar todas las filas del dataframe creado\n",
    "pd.set_option('display.max_rows', df_data.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se visualiza el dataset completo\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpieza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se detectan todos los missing values existentes en el dataset y se aplican distintas estrategias de imputación de la variable etiquetada.\n",
    "\n",
    "Se realiza una imputación de NAN de la variable Salinidad_guadalhorce usando regresion logística.\n",
    "\n",
    "También se limpia el dataset de las columnas vacias, carácteres inadecuados y se cambia el separador decimal de \",\" a \".\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podemos visutalizar la cantidad de missing values presentes en el dataset (iterar para ver todas las columnas)\n",
    "msno.matrix(df_data.iloc[:,25:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porcentaje de NAN en la variable etiquetada\n",
    "salinidad_mezcla_percentage = 100*df_data['Salinidad_mezcla'].isnull().sum()/len(df_data['Salinidad_mezcla'])\n",
    "salinidad_mezcla_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las columnas con todos los valores a 0\n",
    "df_data=df_data.drop(df.columns[25], axis=1)\n",
    "df_data=df_data.drop(df.columns[71], axis=1)\n",
    "df_data=df_data.drop(df.columns[72], axis=1)\n",
    "df_data=df_data.drop(df.columns[73], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea la variable df_dam que contiene todos los datos de la explotación de las presas del valle del guadalhorce\n",
    "df_dam=df_data\n",
    "#Se elimina un valor que está dando error\n",
    "indexNames = df_dam[df_dam['Salinidad_mezcla'] == '1,170' ].index\n",
    "df_dam.drop(indexNames , inplace=True)\n",
    "\n",
    "#Ajustamos el tipo de la variable Salinidad_mezcla para poder procesarla\n",
    "df_dam['Salinidad_mezcla']=df_dam['Salinidad_mezcla'].astype(str).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################-------------------------IMPUTACIÓN NAN EN SALINIDAD MEZCLA------------------###################\n",
    "\n",
    "#Activar solamente un tipo de imputación.\n",
    "\n",
    "#1) BORRAR LAS FILAS QUE CONTIENEN NAN\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "\n",
    "#2) REGRESOR LINEAL PARA PREDECIR LOS VALORES DE NAN\n",
    "#cols_sm=[\"Cota_guadalhorce\",\"Cota_guadalteba\",\"Volumen_total_guadalhorce_guadalteba_conde\",\"Desembalse_total_guadalhorce_guadalteba_conde\"\n",
    "      #,\"Salinidad_mezcla\"]\n",
    "\n",
    "#df_sm=df_dam[cols_sm]\n",
    "#test_df_sm=df_sm[df_sm[\"Salinidad_mezcla\"].isnull()]\n",
    "#df_sm=df_sm.dropna()\n",
    "\n",
    "#ynan_train_sm=df_sm[\"Salinidad_mezcla\"]\n",
    "#Xnan_train_sm=df_sm.drop(\"Salinidad_mezcla\",axis=1)\n",
    "#Xnan_test_sm=test_df_sm.drop(\"Salinidad_mezcla\",axis=1)\n",
    "\n",
    "#Recurrimos a la regresión logistica para predecir el valor de los NAN.\n",
    "#lr_sm=LinearRegression()\n",
    "#lr_sm.fit(Xnan_train_sm, ynan_train_sm)\n",
    "#ynan_pred_sm=lr_sm.predict(Xnan_test_sm)\n",
    "\n",
    "#Se sustituyen los valores NAN por sus predicciones\n",
    "#df_dam.loc[df_dam.Salinidad_mezcla.isnull(),'Salinidad_mezcla']=ynan_pred_sm\n",
    "\n",
    "#3) IMPUTACIÓN CON MEDIA\n",
    "#df_dam['Salinidad_mezcla'].fillna((df_dam['Salinidad_mezcla'].mean()),inplace=True)\n",
    "\n",
    "#4) IMPUTACIÓN CON MEDIANA (con este obtenemos mas que con la predicción)\n",
    "#df_dam['Salinidad_mezcla'].fillna((df_dam['Salinidad_mezcla'].median()),inplace=True)\n",
    "\n",
    "#5) IMPUTACIÓN CON ROLLING MEAN (y eliminación de los NANs restantes)\n",
    "#window = 30 # 30 dias de observación\n",
    "#df_dam['Salinidad_mezcla'].fillna((df_dam['Salinidad_mezcla'].rolling(window,min_periods=1,).mean()),inplace=True)\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#6) IMPUTACIÓN CON ROLLING MEDIAN\n",
    "#window = 30 # 30 dias de observación\n",
    "#df_dam['Salinidad_mezcla'].fillna((df_dam['Salinidad_mezcla'].rolling(window,min_periods=1,).median()),inplace=True)\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#7) IMPUTACIÓN CON INTERPOLACIÓN LINEAL\n",
    "df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='linear')\n",
    "\n",
    "#8) IMPUTACIÓN CON INTERPOLACIÓN CUADRATICA\n",
    "#df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='quadratic')\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#9) IMPUTACIÓN CON INTERPOLACIÓN CUBICA\n",
    "#df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='cubic')\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#10) IMPUTACIÓN CON INTERPOLACIÓN AKIMA\n",
    "#df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='akima')\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#11) IMPUTACIÓN CON INTERPOLACIÓN SPLINE\n",
    "#df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='spline',order=3)\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se va usa unar regeresion logistica para calucular el valor de los NAN de la variable \"Salinidad_guadalhorce\"\n",
    "#Planteamos dos escenarios en la variable \"Salinidad_mezcla\", quitando los NAN y sustituyendo por la media\n",
    "#Implementamos por un lado KFold cross-validation y, posteriormente GridSearchCV\n",
    "\n",
    "#Se va usa unar regeresion logistica para calucular el valor de los NAN de la variable \"Salinidad_guadalhorce\"\n",
    "\n",
    "#Usamos un regresor lineal para predecir los valores NAN de la varibale Salinidad_guadalhorce.\n",
    "cols=[\"Cota_guadalhorce\",\"Salinidad_mezcla\",\"Salinidad_guadalhorce\"]\n",
    "\n",
    "#Se pueden meter mas variables como \"Variacion_volumen_guadalhorce\", \"Volumen_evaporado_guadalhorce\" (esta ultima\n",
    "#habria que quitarle los puntos y pasarla a float antes de meterla en el dataset de predicción)\n",
    "df=df_dam[cols]\n",
    "test_df=df[df[\"Salinidad_guadalhorce\"].isnull()]\n",
    "df=df.dropna()\n",
    "\n",
    "ynan_train=df[\"Salinidad_guadalhorce\"]\n",
    "Xnan_train=df.drop(\"Salinidad_guadalhorce\",axis=1)\n",
    "Xnan_test=test_df.drop(\"Salinidad_guadalhorce\",axis=1)\n",
    "\n",
    "#Recurrimos a la regresión logistica para predecir el valor de los NAN.\n",
    "lr=LinearRegression()\n",
    "lr.fit(Xnan_train, ynan_train)\n",
    "ynan_pred=lr.predict(Xnan_test)\n",
    "\n",
    "#Se sustituyen los valores NAN por sus predicciones\n",
    "df_dam.loc[df_dam.Salinidad_guadalhorce.isnull(),'Salinidad_guadalhorce']=ynan_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porcentaje de NAN trás la imputación\n",
    "salinidad_mezcla_na_percentage = 100*df_dam['Salinidad_mezcla'].isnull().sum()/len(df_dam['Salinidad_mezcla'])\n",
    "salinidad_mezcla_na_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Discretización de la variable etiquetada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La discretización de datos es el proceso de establecer varios puntos de corte para atributos con valores numéricos continuos con el fin de obtener valores enteros o discretos de dichos atributos.\n",
    "\n",
    "En este caso, se ha contado con la Junta de Andalucía para realizar la asignación de intervalos:\n",
    "\n",
    "-\tMezcla de aguas no salina: 0 ≤  Salinidad_mezcla ≤ 900: Salinidad_mezcla = 0 \n",
    "\n",
    "-\tMezcla de aguas salina: Salinidad_mezcla > 900: Salinidad_mezcla = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discretiza la variable Salinidad_mezcla en 1 y 0. 0 para todos los valores por debajo de 900 y 1 para todos los\n",
    "#valores por encima\n",
    "\n",
    "#En este caso se ha subido a 900. \n",
    "bins=[0,900,5000]\n",
    "df_dam['Salinidad_mezcla_tag'] = pd.cut(df_dam['Salinidad_mezcla'], bins, labels=[0,1])\n",
    "\n",
    "#Sustituimos todos los valores NaN del df por 0\n",
    "df_dam=df_dam.fillna(0)\n",
    "#Borra la columna Salinidad_mezcla después de categorizarla\n",
    "df_dam=df_dam.drop(['Salinidad_mezcla'],axis=1)\n",
    "\n",
    "#Separamos la variable etiquetada del resto de dataset\n",
    "df_dam_y=df_dam['Salinidad_mezcla_tag']\n",
    "df_dam_x=df_dam.drop(df_dam[['Salinidad_mezcla_tag','Fecha','Hora','Horas_abastecimiento','Horas_ecologico',\n",
    "                       'Horas_riego','Tiempo_desembalse_guadalhorce','Tiempo_desembalse_guadalteba',\n",
    "                        'Volumen_central-reversible_tajo',\n",
    "                        'Variacion_total_gaitanejo_tajo','Toneladas_sal_guadalhorce',\n",
    "                        'Agua_sobrante','Toneladas_sal_mezcla']],axis=1)\n",
    "\n",
    "\n",
    "#Eliminamos las comas que han quedado y algunos caracteres especiales\n",
    "for i in range(len(df_dam_x.columns)):\n",
    "    col = df_dam_x.columns[i]\n",
    "    if df_dam_x[col].dtypes==object:\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace(',','.')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('#ÁREF!','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('V','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('VAR','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('var','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('0AR','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('12;0','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('9:00','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('-0.073','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('1.6-1.5','0')\n",
    "\n",
    "#Convertimos algunas variables de \"object\" a \"float\"        \n",
    "df_dam_x['Caudal_desembalse_guadalhorce']=df_dam_x['Caudal_desembalse_guadalhorce'].astype(float)\n",
    "df_dam_x['Volumen_evaporado_guadalhorce']=df_dam_x['Volumen_evaporado_guadalhorce'].astype(float)\n",
    "df_dam_x['Volumen_total_gaitanejo_tajo']=df_dam_x['Volumen_total_gaitanejo_tajo'].astype(float)\n",
    "df_dam_x['Caudal_desembalse_guadalteba']=df_dam_x['Caudal_desembalse_guadalteba'].astype(float)\n",
    "df_dam_x['Caudal_abastencimiento']=df_dam_x['Caudal_abastencimiento'].astype(float)\n",
    "df_dam_x['Lluvia']=df_dam_x['Lluvia'].astype(float)        \n",
    "\n",
    "#Convertimos los NAN a 0 de algunas variables, trás la conversión a float\n",
    "df_dam_x['Caudal_desembalse_guadalhorce']=df_dam_x['Caudal_desembalse_guadalhorce'].fillna(0)\n",
    "df_dam_x['Volumen_evaporado_guadalhorce']=df_dam_x['Volumen_evaporado_guadalhorce'].fillna(0)\n",
    "df_dam_x['Volumen_total_gaitanejo_tajo']=df_dam_x['Volumen_total_gaitanejo_tajo'].fillna(0)\n",
    "df_dam_x['Caudal_desembalse_guadalteba']=df_dam_x['Caudal_desembalse_guadalteba'].fillna(0)\n",
    "df_dam_x['Caudal_abastencimiento']=df_dam_x['Caudal_abastencimiento'].fillna(0)\n",
    "df_dam_x['Lluvia']=df_dam_x['Lluvia'].fillna(0)\n",
    "\n",
    "#Vamos a normalizar todas las variables de df_dam_x usando MinMaxScaler.\n",
    "scaler=MinMaxScaler()\n",
    "norm_df=scaler.fit_transform(df_dam_x)\n",
    "\n",
    "#Se vuelve crear un dataframe ya que al normalizar se realiza una conversión a numpy.ndarray\n",
    "norm_df_dam_x=pd.DataFrame(norm_df,columns=list(df_dam_x.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de ver cuales son las variables que mas influyen en la salinidad de la mezcla vamos a obtener los 20 atributos mas relevantes a través de SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features selection con SelectKBest y ANOVA\n",
    "\n",
    "#Extraemos los nombres de las variables\n",
    "features_list=list(norm_df_dam_x.columns)\n",
    "\n",
    "#Aplicación de SelectKBest con el método ANOVA para seleccionar 20 variables mas influyentes\n",
    "features_test = SelectKBest(score_func=f_classif, k=20)\n",
    "fit = features_test.fit(norm_df_dam_x, df_dam_y)\n",
    "\n",
    "set_printoptions(precision=4)\n",
    "print(fit.scores_)\n",
    "#Los valores nan corresponden a las variables \"Filtraciones_guadalhorce\",\"Filtraciones_guadalteba\" y \"Min_ecologico\".\n",
    "#Resultan nan porque son constantes en el caso de las dos primeras, y en el caso de Min_ecologico, todos los valores\n",
    "#son 0, aunque al incio no se detecten como tal.\n",
    "features = fit.transform(norm_df_dam_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se imprimen las 20 variables mas relevantes\n",
    "df_scores=pd.DataFrame(fit.scores_)\n",
    "df_columns=pd.DataFrame(features_list)\n",
    "\n",
    "feature_scores = pd.concat([df_columns, df_scores],axis=1)\n",
    "feature_scores.columns = ['Feature_Name','Score']  \n",
    "print(feature_scores.nlargest(20,'Score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Aplicación de métodos de ML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado el conjunto de datos resultante de la fase de preprocesado, el siguiente paso que se pretende resolver, con la aplicación de los distintos métodos de aprendizaje automático, un problema de clasificación binaria de la variable Salinidad_mezcla. En este tipo de problemas se pretende encontrar un predictor o clasificador con el fin de predecir la clase de la variable etiquetada, en este caso, predecir la clase 0, correspondiente con muestras de agua no salina, o la clase 1, correspondiente con muestras de agua salina "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se separan de forma aleatoria los conjuntos de entrenamiento y test según la regla 85%-15%.\n",
    "X_train_gs, X_test_gs, y_train_gs, y_test_gs = train_test_split(df_dam_feat, df_dam_y, test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta celda es usada para visualizar las fronteras de decisión.\n",
    "#Se separan los conjuntos considerando únicamente las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\"\n",
    "#Aqui no se usa cross-validation por lo tanto test set es de 30%\n",
    "bound_df=df_dam_feat[['Cota_guadalhorce','Salinidad_guadalhorce']]\n",
    "X_train_bound, X_test_bound, y_train_bound, y_test_bound = train_test_split(bound_df, df_dam_y, \n",
    "                                                                            test_size=0.10,random_state=42)\n",
    "\n",
    "#Se crea la siguiente función para visualizar con los distintos algoritomos las fronteras de decisión\n",
    "x_min, x_max = df_dam_feat['Cota_guadalhorce'].min()-0.1, df_dam_feat['Cota_guadalhorce'].max()+0.1\n",
    "y_min, y_max = df_dam_feat['Salinidad_guadalhorce'].min()-0.1, df_dam_feat['Salinidad_guadalhorce'].max()+0.1  \n",
    "\n",
    "def plot_decision_boundaries(x, y, labels, model, \n",
    "                             x_min=x_min, \n",
    "                             x_max=x_max, \n",
    "                             y_min=y_min, \n",
    "                             y_max=y_max, \n",
    "                             grid_step=0.02):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n",
    "                         np.arange(y_min, y_max, grid_step))\n",
    "    \n",
    "    # Predecimos el classifier con los valores de la meshgrid.\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "\n",
    "    # Hacemos reshape para tener el formato correcto.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Seleccionamos una paleta de color.\n",
    "    arr = plt.cm.coolwarm(np.arange(plt.cm.coolwarm.N))\n",
    "    arr_hsv = mpl.colors.rgb_to_hsv(arr[:,0:3])\n",
    "    arr_hsv[:,2] = arr_hsv[:,2] * 1.5\n",
    "    arr_hsv[:,1] = arr_hsv[:,1] * .5\n",
    "    arr_hsv = np.clip(arr_hsv, 0, 1)\n",
    "    arr[:,0:3] = mpl.colors.hsv_to_rgb(arr_hsv) \n",
    "    my_cmap = ListedColormap(arr)\n",
    "    \n",
    "    # Hacemos el gráfico de las fronteras de decisión.\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=my_cmap)\n",
    "\n",
    "    # Añadimos los punts.\n",
    "    ax.scatter(x, y, c=labels, cmap='coolwarm')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Arboles de decisión "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construimos el vector de hiperparametros\n",
    "m_depth=list(range(4,20))\n",
    "min_samp=[2,10,20,50,100]\n",
    "param_grid= dict(max_depth=m_depth, min_samples_split=min_samp)\n",
    "param_grid\n",
    "\n",
    "#A la función GridSearchCV le pasamos el estimador y el vector de hiperparámetros. Además fijamos el parámetro\n",
    "#de cross-validation a 5, así el conjunto de validación será un 17% y el conjunto de entrenamiento un 68%. \n",
    "#Ya que deseamos medir la precisión del modelo, se lo indicamos a través de scoring='accuracy'. \n",
    "#Por último, se usa n_jobs=-1 para ejecutar los procesos en paralelo.\n",
    "gscv_dt=GridSearchCV(tree.DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1,scoring='accuracy')\n",
    "\n",
    "#Se entrena el clasificador\n",
    "gscv_dt_result=gscv_dt.fit(X_train_gs,y_train_gs)\n",
    "\n",
    "#Se imprimen los mejores parámetros hallados\n",
    "print(gscv_dt_result.best_score_)\n",
    "print(gscv_dt_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construimos el dataframe con el que se contruirá el heatmap para visualizar los mejores parámetros resultantes \n",
    "#de la etapa de tunning.\n",
    "sc=gscv_dt_result.cv_results_['mean_test_score']\n",
    "param=gscv_dt_result.cv_results_['params']\n",
    "gscv_dt_result.cv_results_\n",
    "aux1_=pd.DataFrame(param)\n",
    "aux2_=pd.DataFrame({'Accuracy':sc})\n",
    "df_gs_dt=pd.concat([aux1_,aux2_],axis=1)\n",
    "df_gs_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a visualizar un heatmap con los valores de gridsearch\n",
    "hmap_dt = pd.pivot_table(df_gs_dt, values='Accuracy', \n",
    "                     index=['max_depth'], \n",
    "                     columns='min_samples_split')\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(hmap_dt,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos el poder predictor del clasificador hallado (best_estimator_) en el conjunto de test\n",
    "y_pred_dt = gscv_dt_result.best_estimator_.predict(X_test_gs)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_gs, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representamos el arbol de decisión resultante del mejor predictor\n",
    "features_list=list(df_dam_feat.columns)\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "_=tree.plot_tree((gscv_dt_result.best_estimator_),feature_names=features_list,class_names=['0','1'],\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La matriz de confusion quedaría\n",
    "confusion_matrix(y_test_gs, y_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hallamos un predictor usando únicamente las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\" para visualizar\n",
    "#las fronteras de decisión de estas dos variables al clasificar \"Salinidad_mezcla\"\n",
    "#A continuación vamos a crear y a entrenar un nuevo DecisionTreeClassifier\n",
    "clf_dt_bound=tree.DecisionTreeClassifier()\n",
    "\n",
    "#Entrenamos el árbol de decisión\n",
    "clf_dt_bound=clf_dt_bound.fit(X_train_bound,y_train_bound)\n",
    "\n",
    "#Y mostramos la precisión del clasificador hallado\n",
    "y_pred = clf_dt_bound.predict(X_test_bound)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_bound, y_pred))\n",
    "\n",
    "#Mostramos las fronteras decisión cuando seleccionamos las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\"\n",
    "plot_decision_boundaries(X_train_bound['Cota_guadalhorce'], X_train_bound['Salinidad_guadalhorce'],y_train_bound,clf_dt_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construimos el vector de hiperparámetros\n",
    "m_depth_rf=list(range(6,20))\n",
    "num_est=[10,50,100,200]\n",
    "param_grid_rf= dict(max_depth=m_depth_rf, n_estimators=num_est)\n",
    "param_grid_rf\n",
    "\n",
    "#A la función GridSearchCV le pasamos el estimador y el vector de hiperparámetros. Además fijamos el parámetro\n",
    "#de cross-validation a 5, así el conjunto de validación será un 17% y el conjunto de entrenamiento un 68%. \n",
    "#Ya que deseamos medir la precisión del modelo, se lo indicamos a través de scoring='accuracy'. \n",
    "#Por último, se usa n_jobs=-1 para ejecutar los procesos en paralelo.\n",
    "gscv_rf=GridSearchCV(RandomForestClassifier(),param_grid_rf, cv=5, n_jobs=-1,scoring='accuracy')\n",
    "\n",
    "#Se entrena el clasificador\n",
    "gscv_result_rf=gscv_rf.fit(X_train_gs,y_train_gs)\n",
    "\n",
    "#Se imprimen los mejores hiperparámetros\n",
    "print(gscv_result_rf.best_score_)\n",
    "print(gscv_result_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construimos el dataframe con el que se contruirá el heatmap\n",
    "sc_rf=gscv_result_rf.cv_results_['mean_test_score']\n",
    "param_rf=gscv_result_rf.cv_results_['params']\n",
    "gscv_result_rf.cv_results_\n",
    "aux1_rf=pd.DataFrame(param_rf)\n",
    "aux2_rf=pd.DataFrame({'Accuracy':sc_rf})\n",
    "df_gs_rf=pd.concat([aux1_rf,aux2_rf],axis=1)\n",
    "df_gs_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se imprime el heatmap\n",
    "hmap_rf = pd.pivot_table(df_gs_rf, values='Accuracy', \n",
    "                     index=['max_depth'], \n",
    "                     columns='n_estimators')\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(hmap_rf,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos el poder predictor del clasificador hallado (best_estimator_)\n",
    "y_pred_rf = gscv_result_rf.best_estimator_.predict(X_test_gs)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_gs, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La matriz de confusion quedaría\n",
    "confusion_matrix(y_test_gs, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hallamos un predictor usando únicamente las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\" para visualizar\n",
    "#las fronteras de decisión de estas dos variables al clasificar \"Salinidad_mezcla\"\n",
    "clf_rf_bound=RandomForestClassifier(max_depth=14, n_estimators=50)\n",
    "\n",
    "#Entrenamos el bosque de decisión\n",
    "clf_rf_bound=clf_rf_bound.fit(X_train_bound,y_train_bound)\n",
    "\n",
    "#Y mostramos la precisión del clasificador hallado\n",
    "y_pred_rf = clf_rf_bound.predict(X_test_bound)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_bound, y_pred_rf))\n",
    "\n",
    "#Mostramos las fronteras decisión cuando seleccionamos las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\"\n",
    "plot_decision_boundaries(X_train_bound['Cota_guadalhorce'], X_train_bound['Salinidad_guadalhorce'],y_train_bound,clf_rf_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Maquinas de Soporte Vectorial (SVM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los vectores de hiperparámetros que se usará en GridSearch\n",
    "v_C=[0.01,0.1,1,10,50,100,200]\n",
    "v_gamma=[0.001,0.01,0.1,1,10]\n",
    "param_grid_svc= dict(C=v_C, gamma=v_gamma, kernel=['rbf'])\n",
    "\n",
    "#A la función GridSearchCV le pasamos el estimador y el vector de hiperparámetros. Además fijamos el parámetro\n",
    "#de cross-validation a 5, así el conjunto de validación será un 17% y el conjunto de entrenamiento un 68%. \n",
    "#Ya que deseamos medir la precisión del modelo, se lo indicamos a través de scoring='accuracy'. \n",
    "#Por último, se usa n_jobs=-1 para ejecutar los procesos en paralelo.\n",
    "gscv_svc=GridSearchCV(SVC(),param_grid_svc, cv=5, n_jobs=-1,scoring='accuracy')\n",
    "\n",
    "#Se entrena el clasificador\n",
    "gscv_result_svc=gscv_svc.fit(X_train_gs,y_train_gs)\n",
    "\n",
    "print(gscv_result_svc.best_score_)\n",
    "print(gscv_result_svc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Construimos el dataframe con el que se contruirá el heatmap\n",
    "sc_svc=gscv_result_svc.cv_results_['mean_test_score']\n",
    "param_svc=gscv_result_svc.cv_results_['params']\n",
    "gscv_result_svc.cv_results_\n",
    "aux1_svc=pd.DataFrame(param_svc)\n",
    "aux2_svc=pd.DataFrame({'Accuracy':sc_svc})\n",
    "df_gs_svc=pd.concat([aux1_svc,aux2_svc],axis=1)\n",
    "df_gs_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se imprime el heatmap\n",
    "hmap_svc = pd.pivot_table(df_gs_svc, values='Accuracy', \n",
    "                     index=['C'], \n",
    "                     columns='gamma')\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(hmap_svc,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos el poder predictor del clasificador hallado (best_estimator_)\n",
    "y_pred_svc = gscv_result_svc.best_estimator_.predict(X_test_gs)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_gs, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La matriz de confusion quedaría\n",
    "confusion_matrix(y_test_gs, y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hallamos un predictor usando únicamente las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\" para visualizar\n",
    "#las fronteras de decisión de estas dos variables al clasificar \"Salinidad_mezcla\"\n",
    "clf_svc_bound=SVC(C=10,gamma=1,kernel='rbf', probability=True)\n",
    "\n",
    "#Entrenamos el árbol de decisión\n",
    "clf_svc_bound=clf_svc_bound.fit(X_train_bound,y_train_bound)\n",
    "\n",
    "#Y mostramos la precisión del clasificador hallado\n",
    "y_pred_svc = clf_svc_bound.predict(X_test_bound)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_bound, y_pred_svc))\n",
    "\n",
    "#Mostramos las fronteras decisión cuando seleccionamos las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\"\n",
    "plot_decision_boundaries(X_train_bound['Cota_guadalhorce'], X_train_bound['Salinidad_guadalhorce'],y_train_bound,clf_svc_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construimos el vector de hiperparámetros\n",
    "l_rate=[0.01,0.1,1,2]\n",
    "n_estimators_boost=[10,50,100,200]\n",
    "param_grid_ab= dict( learning_rate=l_rate,n_estimators=n_estimators_boost)\n",
    "\n",
    "#A la función GridSearchCV le pasamos el estimador y el vector de hiperparámetros. Además fijamos el parámetro\n",
    "#de cross-validation a 5, así el conjunto de validación será un 17% y el conjunto de entrenamiento un 68%. \n",
    "#Ya que deseamos medir la precisión del modelo, se lo indicamos a través de scoring='accuracy'. \n",
    "#Por último, se usa n_jobs=-1 para ejecutar los procesos en paralelo.\n",
    "gscv_ab=GridSearchCV(AdaBoostClassifier(),param_grid_ab, cv=5, n_jobs=-1,scoring='accuracy')\n",
    "\n",
    "#Al igual que en el caso anterior, usamos X e y, en lugar de X_train e y_train ya que usando cross-validation \n",
    "#nos aseguramos que el algoritmo separa el el conjunto de datos en 4 subconjuntos, eligiendo en cada iteracion \n",
    "#un subconjunto para el train set [1].\n",
    "gscv_result_ab=gscv_ab.fit(X_train_gs,y_train_gs)\n",
    "\n",
    "print(gscv_result_ab.best_score_)\n",
    "print(gscv_result_ab.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construimos el dataframe con el que se contruirá el heatmap\n",
    "sc_ab=gscv_result_ab.cv_results_['mean_test_score']\n",
    "param_ab=gscv_result_ab.cv_results_['params']\n",
    "gscv_result_ab.cv_results_\n",
    "aux1_ab=pd.DataFrame(param_ab)\n",
    "aux2_ab=pd.DataFrame({'Accuracy':sc_ab})\n",
    "df_gs_ab=pd.concat([aux1_ab,aux2_ab],axis=1)\n",
    "df_gs_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmap_ab = pd.pivot_table(df_gs_ab, values='Accuracy', \n",
    "                     index=['learning_rate'], \n",
    "                     columns='n_estimators')\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(hmap_ab,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos el poder predictor del clasificador hallado (best_estimator_)\n",
    "y_pred_ab = gscv_result_ab.best_estimator_.predict(X_test_gs)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_gs, y_pred_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La matriz de confusion quedaría\n",
    "confusion_matrix(y_test_gs, y_pred_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hallamos un predictor usando únicamente las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\"\n",
    "#A continuación vamos a crear y a entrenar un nuevo DecisionTreeClassifier\n",
    "clf_ab_bound=GradientBoostingClassifier(learning_rate=0.1, n_estimators=100)\n",
    "\n",
    "#Entrenamos el árbol de decisión\n",
    "clf_ab_bound=clf_ab_bound.fit(X_train_bound,y_train_bound)\n",
    "\n",
    "#Y mostramos la precisión del clasificador hallado\n",
    "y_pred_ab = clf_ab_bound.predict(X_test_bound)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_bound, y_pred_ab))\n",
    "\n",
    "#Mostramos las fronteras decisión cuando seleccionamos las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\"\n",
    "plot_decision_boundaries(X_train_bound['Cota_guadalhorce'], X_train_bound['Salinidad_guadalhorce'],y_train_bound,clf_ab_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Análisis de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra la precisión de clasificación de la variable Salinidad_mezcla hallada para cada método de aprendizaje automático, en los conjuntos de entrenamiento y validación, y para cada método de imputación de missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_methods=[\"Imputation Method\",\"Decision Trees\", \"Random Forest\",\"Support Vector Machine\", \"AdaBoost\",\"ANN\"]\n",
    "imputation_methods=['Drop NAN','Mean','Median','Rolling Mean','Rolling Median','Linear Interpolation','Quadratic Interpolation',\n",
    "                   'Cubic Interpolation','Akima Interpolation','Spline Interpolation','Linear Regressor Predictor']\n",
    "                   \n",
    "result_mean=[87.26,89.46,87.26,87.96,85.26]\n",
    "result_median=[88.57,90.48,88.49,90.52]\n",
    "result_rolling_mean=[90.09,92.87,88.96,91.65]\n",
    "result_rolli_median=[88.91,92.56,89.74,91.74],\n",
    "result_inter_linear=[90.89,94.02,91.17,93.04]\n",
    "result_inter_quadratic=[89.96,92.13,88.77,90.86]\n",
    "result_inter_cubic=[89.80,92.50,88.00,90.99]\n",
    "result_inter_akima=[90.78,93.93,91.03,93.28]\n",
    "#result_inter_pol_3=[89.92,92.50,88.00,90.86]\n",
    "result_inter_spline=[89.14,92.31,87.31,90.03]\n",
    "\n",
    "result_desicion_trees=['87.26','88.32','88.36','89.74','89.52','91.21','89.14','89.06','91.27','89.83','87.63']\n",
    "result_random_forest=['89.46','88.97','90.36','92.56','92.82','93.94','91.89','92.05','93.85','92.15','89.79']\n",
    "result_svm=['87.26','88.16','88.45','88.61','89.61','91.01','88.73','88.12','90.62','87.18','88.32']\n",
    "result_ada_boost=['87.96','86.82','88.53','90.04','89.43','91.37','88.61','89.06','91.11','88.04','88.45']\n",
    "result_ann=['85.26','86.21','87.83','83.70','83.00','86.66','82.75','82.30','85.87','81.61','86.41']\n",
    "\n",
    "df_result=pd.DataFrame(list(zip(imputation_methods,result_desicion_trees,result_random_forest,result_svm,result_ada_boost,result_ann))\n",
    "                       ,columns=ml_methods)\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_desicion_trees=['84.18','86.89','84.82','89.68','89.92','93.33','89.32','88.63','90.95','89.88','88.96']\n",
    "result_test_random_forest=['88.70','87.81','90.11','92.13','94.10','95.63','92.80','93.50','94.89','93.33','90.57']\n",
    "result_test_svm=['85.31','86.66','88.96','87.96','89.92','93.10','87.47','88.16','89.32','87.81','87.12']\n",
    "result_test_ada_boost=['89.26','86.43','87.81','88.20','87.96','93.10','90.48','89.55','92.11','88.96','88.50']\n",
    "result_test_ann=['81.35','85.51','87.81','82.55','80.83','87.58','85.38','84.91','87.00','84.13','86.20']\n",
    "\n",
    "df_test_result=pd.DataFrame(list(zip(imputation_methods,result_test_desicion_trees,result_test_random_forest,result_test_svm,result_test_ada_boost,result_test_ann))\n",
    "                       ,columns=ml_methods)\n",
    "\n",
    "df_test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 Predicción de Salinidad_mezcla en función de otras variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se creará un dataframe en el que solo variará los valores de la variable \"Salinidad_guadalhorce\" y \n",
    "el resto de variables permanecerá constante. Este dataframe servirá como entrada al predictor con más capacidad \n",
    "predictiva hallado anteriormente con el objetivo de analizar la clasificación de la variable \"Salinidad_mezcla\" en\n",
    "función de la variación de \"Salinidad_guadalhorce\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicción de la Salinidad_mezcla en función de Salinidad_guadalhorce\n",
    "\n",
    "#Extraemos los valores máximos y mínimos de la variable \"Salinidad_guadalhorce\" y elegimos 102 elementos para evaluar\n",
    "#el predictor. Estos elementos se normalizan ya que el predictor está optimizado para valores normalizados.\n",
    "max_val=[max(df_dam_x[\"Salinidad_guadalhorce\"])]\n",
    "max_val=pd.DataFrame(max_val)\n",
    "min_val=[min(df_dam_x[\"Salinidad_guadalhorce\"])]\n",
    "min_val=pd.DataFrame(min_val)\n",
    "v_salt=range(4,3991,40)\n",
    "v_salt=pd.DataFrame(v_salt)\n",
    "v_salt=pd.concat([min_val, v_salt]).reset_index(drop = True) \n",
    "v_salt=v_salt.append(max_val,ignore_index=True)\n",
    "v_salt=v_salt.rename(columns={0:\"Salinidad_guadalhorce\"})\n",
    "scaler=MinMaxScaler()\n",
    "v_salt_norm=scaler.fit_transform(v_salt)\n",
    "v_salt_norm=pd.DataFrame(v_salt_norm,columns=[\"Salinidad_guadalhorce\"])\n",
    "\n",
    "#Elegimos la mediana de cada variable resultante del proceso de reducción de dimensionalidad\n",
    "column_names=[\"Caudal_riego\",\"Volumen_riego\",\"Volumen_evaporado_guadalteba\",\"Cota_guadalhorce\",\"Cota_guadalteba\",\n",
    "             \"Volumen_embalsado_guadalhorce\",\"Salinidad_guadalhorce\",\"Volumen_total_guadalhorce_guadalteba\",\n",
    "             \"Temperatura_Max\",\"Volumen_embalsado_guadalteba\",\"Volumen_total_guadalhorce_guadalteba_conde\",\n",
    "             \"Evaporacion\",\"Temperatura_Min\",\"Volumen_total_demandas\",\"Caudal_total_demandas\",\n",
    "             \"Total_volumen_abastecido\",\"Cota_conde\",\"Volumen_desembalse_guadalteba\",\"Volumen_embalsado_conde\",\n",
    "             \"Anyo\"]\n",
    "v_1=df_dam_feat[\"Caudal_riego\"].median()\n",
    "v_2=df_dam_feat[\"Volumen_riego\"].median()\n",
    "v_3=df_dam_feat[\"Volumen_evaporado_guadalteba\"].median()\n",
    "v_4=df_dam_feat[\"Cota_guadalhorce\"].median()\n",
    "v_5=df_dam_feat[\"Cota_guadalteba\"].median()\n",
    "v_6=df_dam_feat[\"Volumen_embalsado_guadalhorce\"].median()\n",
    "v_7=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba\"].median()\n",
    "v_8=df_dam_feat[\"Temperatura_Max\"].median()\n",
    "v_9=df_dam_feat[\"Volumen_embalsado_guadalteba\"].median()\n",
    "v_10=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba_conde\"].median()\n",
    "v_11=df_dam_feat[\"Evaporacion\"].median()\n",
    "v_12=df_dam_feat[\"Temperatura_Min\"].median()\n",
    "v_13=df_dam_feat[\"Volumen_total_demandas\"].median()\n",
    "v_14=df_dam_feat[\"Caudal_total_demandas\"].median()\n",
    "v_15=df_dam_feat[\"Total_volumen_abastecido\"].median()\n",
    "v_16=df_dam_feat[\"Cota_conde\"].median()\n",
    "v_17=df_dam_feat[\"Volumen_desembalse_guadalteba\"].median()\n",
    "v_18=df_dam_feat[\"Volumen_embalsado_conde\"].median()\n",
    "v_19=df_dam_feat[\"Anyo\"].median()\n",
    "\n",
    "v_1=np.full(shape=102,fill_value=v_1)\n",
    "v_1=pd.DataFrame(v_1)\n",
    "v_2=np.full(shape=102,fill_value=v_2)\n",
    "v_2=pd.DataFrame(v_2)\n",
    "v_3=np.full(shape=102,fill_value=v_3)\n",
    "v_3=pd.DataFrame(v_3)\n",
    "v_4=np.full(shape=102,fill_value=v_4)\n",
    "v_4=pd.DataFrame(v_4)\n",
    "v_5=np.full(shape=102,fill_value=v_5)\n",
    "v_5=pd.DataFrame(v_5)\n",
    "v_6=np.full(shape=102,fill_value=v_6)\n",
    "v_6=pd.DataFrame(v_6)\n",
    "v_7=np.full(shape=102,fill_value=v_7)\n",
    "v_7=pd.DataFrame(v_7)\n",
    "v_8=np.full(shape=102,fill_value=v_8)\n",
    "v_8=pd.DataFrame(v_8)\n",
    "v_9=np.full(shape=102,fill_value=v_9)\n",
    "v_9=pd.DataFrame(v_9)\n",
    "v_10=np.full(shape=102,fill_value=v_10)\n",
    "v_10=pd.DataFrame(v_10)\n",
    "v_11=np.full(shape=102,fill_value=v_11)\n",
    "v_11=pd.DataFrame(v_11)\n",
    "v_12=np.full(shape=102,fill_value=v_12)\n",
    "v_12=pd.DataFrame(v_12)\n",
    "v_13=np.full(shape=102,fill_value=v_13)\n",
    "v_13=pd.DataFrame(v_13)\n",
    "v_14=np.full(shape=102,fill_value=v_14)\n",
    "v_14=pd.DataFrame(v_14)\n",
    "v_15=np.full(shape=102,fill_value=v_15)\n",
    "v_15=pd.DataFrame(v_15)\n",
    "v_16=np.full(shape=102,fill_value=v_16)\n",
    "v_16=pd.DataFrame(v_16)\n",
    "v_17=np.full(shape=102,fill_value=v_17)\n",
    "v_17=pd.DataFrame(v_17)\n",
    "v_18=np.full(shape=102,fill_value=v_18)\n",
    "v_18=pd.DataFrame(v_18)\n",
    "v_19=np.full(shape=102,fill_value=v_19)\n",
    "v_19=pd.DataFrame(v_19)\n",
    "df_salt_eval=pd.concat([v_1,v_2,v_3,v_4,v_5,v_6,v_salt_norm,v_7,v_8,v_9,v_10,v_11,v_12,v_13,v_14,v_15,v_16,v_17,v_18\n",
    "                    ,v_19],axis=1)\n",
    "df_salt_eval.columns=column_names\n",
    "\n",
    "#Usamos el mejor predictor sobre el dataframe creado\n",
    "\n",
    "#Deshacemos la normalización en la variable \"Salinidad_guadalhorce\" para ver la clasificación de \"Salinidad_mezcla\"\n",
    "#en función de los valores reales de \"Salinidad_guadalhorce\"\n",
    "salt_ghorce_eval=scaler.inverse_transform(v_salt_norm)\n",
    "result_eval=gscv_result_rf.best_estimator_.predict(df_salt_eval)\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.scatter(salt_ghorce_eval,result_eval)\n",
    "plt.xlabel(\"Salinidad de la presa Guadalhorce\")\n",
    "plt.ylabel(\"Predicción de la salinidad de la mezcla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicción de la Salinidad_mezcla en función de Cota_guadalhorce\n",
    "\n",
    "#Extraemos los valores máximos y mínimos de la variable \"Cota_guadalhorce\" y elegimos 102 elementos para evaluar\n",
    "#el predictor. Estos elementos se normalizan ya que el predictor está optimizado para valores normalizados.\n",
    "max_val_level=[max(df_dam_x[\"Cota_guadalhorce\"])]\n",
    "max_val_level=pd.DataFrame(max_val_level)\n",
    "min_val_level=[min(df_dam_x[\"Cota_guadalhorce\"])]\n",
    "min_val_level=pd.DataFrame(min_val_level)\n",
    "v_salt_level=np.arange(349, 362.5, 0.135)\n",
    "v_salt_level=pd.DataFrame(v_salt_level)\n",
    "v_salt_level=pd.concat([min_val_level, v_salt_level]).reset_index(drop = True) \n",
    "v_salt_level=v_salt_level.append(max_val_level,ignore_index=True)\n",
    "v_salt_level=v_salt_level.rename(columns={0:\"Cota_guadalhorce\"})\n",
    "scaler=MinMaxScaler()\n",
    "v_salt_level_norm=scaler.fit_transform(v_salt_level)\n",
    "v_salt_level_norm=pd.DataFrame(v_salt_level_norm,columns=[\"Cota_guadalhorce\"])\n",
    "\n",
    "\n",
    "v_1=df_dam_feat[\"Caudal_riego\"].median()\n",
    "v_2=df_dam_feat[\"Volumen_riego\"].median()\n",
    "v_3=df_dam_feat[\"Volumen_evaporado_guadalteba\"].median()\n",
    "v_4=df_dam_feat[\"Salinidad_guadalhorce\"].median()\n",
    "v_5=df_dam_feat[\"Cota_guadalteba\"].median()\n",
    "v_6=df_dam_feat[\"Volumen_embalsado_guadalhorce\"].median()\n",
    "v_7=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba\"].median()\n",
    "v_8=df_dam_feat[\"Temperatura_Max\"].median()\n",
    "v_9=df_dam_feat[\"Volumen_embalsado_guadalteba\"].median()\n",
    "v_10=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba_conde\"].median()\n",
    "v_11=df_dam_feat[\"Evaporacion\"].median()\n",
    "v_12=df_dam_feat[\"Temperatura_Min\"].median()\n",
    "v_13=df_dam_feat[\"Volumen_total_demandas\"].median()\n",
    "v_14=df_dam_feat[\"Caudal_total_demandas\"].median()\n",
    "v_15=df_dam_feat[\"Total_volumen_abastecido\"].median()\n",
    "v_16=df_dam_feat[\"Cota_conde\"].median()\n",
    "v_17=df_dam_feat[\"Volumen_desembalse_guadalteba\"].median()\n",
    "v_18=df_dam_feat[\"Volumen_embalsado_conde\"].median()\n",
    "v_19=df_dam_feat[\"Anyo\"].median()\n",
    "\n",
    "v_1=np.full(shape=102,fill_value=v_1)\n",
    "v_1=pd.DataFrame(v_1)\n",
    "v_2=np.full(shape=102,fill_value=v_2)\n",
    "v_2=pd.DataFrame(v_2)\n",
    "v_3=np.full(shape=102,fill_value=v_3)\n",
    "v_3=pd.DataFrame(v_3)\n",
    "v_4=np.full(shape=102,fill_value=v_4)\n",
    "v_4=pd.DataFrame(v_4)\n",
    "v_5=np.full(shape=102,fill_value=v_5)\n",
    "v_5=pd.DataFrame(v_5)\n",
    "v_6=np.full(shape=102,fill_value=v_6)\n",
    "v_6=pd.DataFrame(v_6)\n",
    "v_7=np.full(shape=102,fill_value=v_7)\n",
    "v_7=pd.DataFrame(v_7)\n",
    "v_8=np.full(shape=102,fill_value=v_8)\n",
    "v_8=pd.DataFrame(v_8)\n",
    "v_9=np.full(shape=102,fill_value=v_9)\n",
    "v_9=pd.DataFrame(v_9)\n",
    "v_10=np.full(shape=102,fill_value=v_10)\n",
    "v_10=pd.DataFrame(v_10)\n",
    "v_11=np.full(shape=102,fill_value=v_11)\n",
    "v_11=pd.DataFrame(v_11)\n",
    "v_12=np.full(shape=102,fill_value=v_12)\n",
    "v_12=pd.DataFrame(v_12)\n",
    "v_13=np.full(shape=102,fill_value=v_13)\n",
    "v_13=pd.DataFrame(v_13)\n",
    "v_14=np.full(shape=102,fill_value=v_14)\n",
    "v_14=pd.DataFrame(v_14)\n",
    "v_15=np.full(shape=102,fill_value=v_15)\n",
    "v_15=pd.DataFrame(v_15)\n",
    "v_16=np.full(shape=102,fill_value=v_16)\n",
    "v_16=pd.DataFrame(v_16)\n",
    "v_17=np.full(shape=102,fill_value=v_17)\n",
    "v_17=pd.DataFrame(v_17)\n",
    "v_18=np.full(shape=102,fill_value=v_18)\n",
    "v_18=pd.DataFrame(v_18)\n",
    "v_19=np.full(shape=102,fill_value=v_19)\n",
    "v_19=pd.DataFrame(v_19)\n",
    "df_level_eval=pd.concat([v_1,v_2,v_3,v_salt_level_norm,v_5,v_6,v_4,v_7,v_8,v_9,v_10,v_11,v_12,v_13,v_14,v_15,v_16,v_17,v_18\n",
    "                    ,v_19],axis=1)\n",
    "df_level_eval.columns=column_names\n",
    "\n",
    "#Usamos el mejor predictor sobre el dataframe creado\n",
    "\n",
    "#Deshacemos la normalización en la variable \"Cota_guadalhorce\" para ver la clasificación de \"Salinidad_mezcla\"\n",
    "#en función de los valores reales de \"Cota_guadalhorce\"\n",
    "level_ghorce_eval=scaler.inverse_transform(v_salt_level_norm)\n",
    "result_eval_level=gscv_result_rf.best_estimator_.predict(df_level_eval)\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.scatter(level_ghorce_eval,result_eval_level)\n",
    "plt.xlabel(\"Cota de la presa Guadalhorce\")\n",
    "plt.ylabel(\"Predicción de la salinidad de la mezcla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicción de la Salinidad_mezcla en función de Volumen_total_guadalhorce_guadalteba_conde\n",
    "\n",
    "#Extraemos los valores máximos y mínimos de la variable \"Volumen_total_guadalhorce_guadalteba_conde\" y elegimos 102 elementos para evaluar\n",
    "#el predictor. Estos elementos se normalizan ya que el predictor está optimizado para valores normalizados.\n",
    "max_val_vol=[max(df_dam_x[\"Volumen_total_guadalhorce_guadalteba_conde\"])]\n",
    "max_val_vol=pd.DataFrame(max_val_vol)\n",
    "min_val_vol=[min(df_dam_x[\"Volumen_total_guadalhorce_guadalteba_conde\"])]\n",
    "min_val_vol=pd.DataFrame(min_val_vol)\n",
    "v_salt_vol=np.arange(109.497, 354.046, 2.45)\n",
    "v_salt_vol=pd.DataFrame(v_salt_vol)\n",
    "v_salt_vol=pd.concat([min_val_vol, v_salt_vol]).reset_index(drop = True) \n",
    "v_salt_vol=v_salt_vol.append(max_val_vol,ignore_index=True)\n",
    "v_salt_vol=v_salt_vol.rename(columns={0:\"Volumen_total_guadalhorce_guadalteba_conde\"})\n",
    "scaler=MinMaxScaler()\n",
    "v_salt_vol_norm=scaler.fit_transform(v_salt_vol)\n",
    "v_salt_vol_norm=pd.DataFrame(v_salt_vol_norm,columns=[\"Volumen_total_guadalhorce_guadalteba_conde\"])\n",
    "\n",
    "\n",
    "v_1=df_dam_feat[\"Caudal_riego\"].median()\n",
    "v_2=df_dam_feat[\"Volumen_riego\"].median()\n",
    "v_3=df_dam_feat[\"Volumen_evaporado_guadalteba\"].median()\n",
    "v_4=df_dam_feat[\"Salinidad_guadalhorce\"].median()\n",
    "v_5=df_dam_feat[\"Cota_guadalteba\"].median()\n",
    "v_6=df_dam_feat[\"Volumen_embalsado_guadalhorce\"].median()\n",
    "v_7=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba\"].median()\n",
    "v_8=df_dam_feat[\"Temperatura_Max\"].median()\n",
    "v_9=df_dam_feat[\"Volumen_embalsado_guadalteba\"].median()\n",
    "v_10=df_dam_feat[\"Cota_guadalhorce\"].median()\n",
    "v_11=df_dam_feat[\"Evaporacion\"].median()\n",
    "v_12=df_dam_feat[\"Temperatura_Min\"].median()\n",
    "v_13=df_dam_feat[\"Volumen_total_demandas\"].median()\n",
    "v_14=df_dam_feat[\"Caudal_total_demandas\"].median()\n",
    "v_15=df_dam_feat[\"Total_volumen_abastecido\"].median()\n",
    "v_16=df_dam_feat[\"Cota_conde\"].median()\n",
    "v_17=df_dam_feat[\"Volumen_desembalse_guadalteba\"].median()\n",
    "v_18=df_dam_feat[\"Volumen_embalsado_conde\"].median()\n",
    "v_19=df_dam_feat[\"Anyo\"].median()\n",
    "\n",
    "v_1=np.full(shape=102,fill_value=v_1)\n",
    "v_1=pd.DataFrame(v_1)\n",
    "v_2=np.full(shape=102,fill_value=v_2)\n",
    "v_2=pd.DataFrame(v_2)\n",
    "v_3=np.full(shape=102,fill_value=v_3)\n",
    "v_3=pd.DataFrame(v_3)\n",
    "v_4=np.full(shape=102,fill_value=v_4)\n",
    "v_4=pd.DataFrame(v_4)\n",
    "v_5=np.full(shape=102,fill_value=v_5)\n",
    "v_5=pd.DataFrame(v_5)\n",
    "v_6=np.full(shape=102,fill_value=v_6)\n",
    "v_6=pd.DataFrame(v_6)\n",
    "v_7=np.full(shape=102,fill_value=v_7)\n",
    "v_7=pd.DataFrame(v_7)\n",
    "v_8=np.full(shape=102,fill_value=v_8)\n",
    "v_8=pd.DataFrame(v_8)\n",
    "v_9=np.full(shape=102,fill_value=v_9)\n",
    "v_9=pd.DataFrame(v_9)\n",
    "v_10=np.full(shape=102,fill_value=v_10)\n",
    "v_10=pd.DataFrame(v_10)\n",
    "v_11=np.full(shape=102,fill_value=v_11)\n",
    "v_11=pd.DataFrame(v_11)\n",
    "v_12=np.full(shape=102,fill_value=v_12)\n",
    "v_12=pd.DataFrame(v_12)\n",
    "v_13=np.full(shape=102,fill_value=v_13)\n",
    "v_13=pd.DataFrame(v_13)\n",
    "v_14=np.full(shape=102,fill_value=v_14)\n",
    "v_14=pd.DataFrame(v_14)\n",
    "v_15=np.full(shape=102,fill_value=v_15)\n",
    "v_15=pd.DataFrame(v_15)\n",
    "v_16=np.full(shape=102,fill_value=v_16)\n",
    "v_16=pd.DataFrame(v_16)\n",
    "v_17=np.full(shape=102,fill_value=v_17)\n",
    "v_17=pd.DataFrame(v_17)\n",
    "v_18=np.full(shape=102,fill_value=v_18)\n",
    "v_18=pd.DataFrame(v_18)\n",
    "v_19=np.full(shape=102,fill_value=v_19)\n",
    "v_19=pd.DataFrame(v_19)\n",
    "df_vol_eval=pd.concat([v_1,v_2,v_3,v_10,v_5,v_6,v_4,v_7,v_8,v_9,v_salt_vol_norm,v_11,v_12,v_13,v_14,v_15,v_16,v_17,v_18\n",
    "                    ,v_19],axis=1)\n",
    "df_vol_eval.columns=column_names\n",
    "\n",
    "column_names=[\"Caudal_riego\",\"Volumen_riego\",\"Volumen_evaporado_guadalteba\",\"Cota_guadalhorce\",\"Cota_guadalteba\",\n",
    "             \"Volumen_embalsado_guadalhorce\",\"Salinidad_guadalhorce\",\"Volumen_total_guadalhorce_guadalteba\",\n",
    "             \"Temperatura_Max\",\"Volumen_embalsado_guadalteba\",\"Volumen_total_guadalhorce_guadalteba_conde\",\n",
    "             \"Evaporacion\",\"Temperatura_Min\",\"Volumen_total_demandas\",\"Caudal_total_demandas\",\n",
    "             \"Total_volumen_abastecido\",\"Cota_conde\",\"Volumen_desembalse_guadalteba\",\"Volumen_embalsado_conde\",\n",
    "             \"Anyo\"]\n",
    "\n",
    "\n",
    "#Usamos el mejor predictor sobre el dataframe creado\n",
    "\n",
    "#Deshacemos la normalización en la variable \"Volumen_total_guadalhorce_guadalteba_conde\" para ver la clasificación de \"Salinidad_mezcla\"\n",
    "#en función de los valores reales de \"Volumen_total_guadalhorce_guadalteba_conde\"\n",
    "vol_eval=scaler.inverse_transform(v_salt_vol_norm)\n",
    "result_eval_vol=gscv_result_rf.best_estimator_.predict(df_vol_eval)\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.scatter(vol_eval,result_eval_vol)\n",
    "plt.xlabel(\"Suma de volumenes embalsados\")\n",
    "plt.ylabel(\"Predicción de la salinidad de la mezcla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicción de la Salinidad_mezcla en función de \"Volumen_desembalse_guadalteba\"\n",
    "\n",
    "#Extraemos los valores máximos y mínimos de la variable \"Volumen_desembalse_guadalteba\" y elegimos 102 elementos para evaluar\n",
    "#el predictor. Estos elementos se normalizan ya que el predictor está optimizado para valores normalizados.\n",
    "max_val_des=[max(df_dam_x[\"Volumen_desembalse_guadalteba\"])]\n",
    "max_val_des=pd.DataFrame(max_val_des)\n",
    "min_val_des=[min(df_dam_x[\"Volumen_desembalse_guadalteba\"])]\n",
    "min_val_des=pd.DataFrame(min_val_des)\n",
    "v_salt_des=np.arange(0.01, 1.626, 0.0162)\n",
    "v_salt_des=pd.DataFrame(v_salt_des)\n",
    "v_salt_des=pd.concat([min_val_des, v_salt_des]).reset_index(drop = True) \n",
    "v_salt_des=v_salt_des.append(max_val_des,ignore_index=True)\n",
    "v_salt_des=v_salt_des.rename(columns={0:\"Volumen_desembalse_guadalteba\"})\n",
    "scaler=MinMaxScaler()\n",
    "v_salt_des_norm=scaler.fit_transform(v_salt_des)\n",
    "v_salt_des_norm=pd.DataFrame(v_salt_des_norm,columns=[\"Volumen_desembalse_guadalteba\"])\n",
    "\n",
    "\n",
    "v_1=df_dam_feat[\"Caudal_riego\"].median()\n",
    "v_2=df_dam_feat[\"Volumen_riego\"].median()\n",
    "v_3=df_dam_feat[\"Volumen_evaporado_guadalteba\"].median()\n",
    "v_4=df_dam_feat[\"Salinidad_guadalhorce\"].median()\n",
    "v_5=df_dam_feat[\"Cota_guadalteba\"].median()\n",
    "v_6=df_dam_feat[\"Volumen_embalsado_guadalhorce\"].median()\n",
    "v_7=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba\"].median()\n",
    "v_8=df_dam_feat[\"Temperatura_Max\"].median()\n",
    "v_9=df_dam_feat[\"Volumen_embalsado_guadalteba\"].median()\n",
    "v_10=df_dam_feat[\"Cota_guadalhorce\"].median()\n",
    "v_11=df_dam_feat[\"Evaporacion\"].median()\n",
    "v_12=df_dam_feat[\"Temperatura_Min\"].median()\n",
    "v_13=df_dam_feat[\"Volumen_total_demandas\"].median()\n",
    "v_14=df_dam_feat[\"Caudal_total_demandas\"].median()\n",
    "v_15=df_dam_feat[\"Total_volumen_abastecido\"].median()\n",
    "v_16=df_dam_feat[\"Cota_conde\"].median()\n",
    "v_17=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba_conde\"].median()\n",
    "v_18=df_dam_feat[\"Volumen_embalsado_conde\"].median()\n",
    "v_19=df_dam_feat[\"Anyo\"].median()\n",
    "\n",
    "v_1=np.full(shape=102,fill_value=v_1)\n",
    "v_1=pd.DataFrame(v_1)\n",
    "v_2=np.full(shape=102,fill_value=v_2)\n",
    "v_2=pd.DataFrame(v_2)\n",
    "v_3=np.full(shape=102,fill_value=v_3)\n",
    "v_3=pd.DataFrame(v_3)\n",
    "v_4=np.full(shape=102,fill_value=v_4)\n",
    "v_4=pd.DataFrame(v_4)\n",
    "v_5=np.full(shape=102,fill_value=v_5)\n",
    "v_5=pd.DataFrame(v_5)\n",
    "v_6=np.full(shape=102,fill_value=v_6)\n",
    "v_6=pd.DataFrame(v_6)\n",
    "v_7=np.full(shape=102,fill_value=v_7)\n",
    "v_7=pd.DataFrame(v_7)\n",
    "v_8=np.full(shape=102,fill_value=v_8)\n",
    "v_8=pd.DataFrame(v_8)\n",
    "v_9=np.full(shape=102,fill_value=v_9)\n",
    "v_9=pd.DataFrame(v_9)\n",
    "v_10=np.full(shape=102,fill_value=v_10)\n",
    "v_10=pd.DataFrame(v_10)\n",
    "v_11=np.full(shape=102,fill_value=v_11)\n",
    "v_11=pd.DataFrame(v_11)\n",
    "v_12=np.full(shape=102,fill_value=v_12)\n",
    "v_12=pd.DataFrame(v_12)\n",
    "v_13=np.full(shape=102,fill_value=v_13)\n",
    "v_13=pd.DataFrame(v_13)\n",
    "v_14=np.full(shape=102,fill_value=v_14)\n",
    "v_14=pd.DataFrame(v_14)\n",
    "v_15=np.full(shape=102,fill_value=v_15)\n",
    "v_15=pd.DataFrame(v_15)\n",
    "v_16=np.full(shape=102,fill_value=v_16)\n",
    "v_16=pd.DataFrame(v_16)\n",
    "v_17=np.full(shape=102,fill_value=v_17)\n",
    "v_17=pd.DataFrame(v_17)\n",
    "v_18=np.full(shape=102,fill_value=v_18)\n",
    "v_18=pd.DataFrame(v_18)\n",
    "v_19=np.full(shape=102,fill_value=v_19)\n",
    "v_19=pd.DataFrame(v_19)\n",
    "\n",
    "\n",
    "column_names=[\"Caudal_riego\",\"Volumen_riego\",\"Volumen_evaporado_guadalteba\",\"Cota_guadalhorce\",\"Cota_guadalteba\",\n",
    "             \"Volumen_embalsado_guadalhorce\",\"Salinidad_guadalhorce\",\"Volumen_total_guadalhorce_guadalteba\",\n",
    "             \"Temperatura_Max\",\"Volumen_embalsado_guadalteba\",\"Volumen_total_guadalhorce_guadalteba_conde\",\n",
    "             \"Evaporacion\",\"Temperatura_Min\",\"Volumen_total_demandas\",\"Caudal_total_demandas\",\n",
    "             \"Total_volumen_abastecido\",\"Cota_conde\",\"Volumen_desembalse_guadalteba\",\"Volumen_embalsado_conde\",\n",
    "             \"Anyo\"]\n",
    "\n",
    "df_des_eval=pd.concat([v_1,v_2,v_3,v_10,v_5,v_6,v_4,v_7,v_8,v_9,v_17,v_11,v_12,v_13,v_14,v_15,v_16,v_salt_des,v_18\n",
    "                    ,v_19],axis=1)\n",
    "df_des_eval.columns=column_names\n",
    "\n",
    "\n",
    "#Usamos el mejor predictor sobre el dataframe creado\n",
    "\n",
    "#Deshacemos la normalización en la variable \"Volumen_desembalse_guadalteba\" para ver la clasificación de \"Salinidad_mezcla\"\n",
    "#en función de los valores reales de \"Volumen_desembalse_guadalteba\"\n",
    "des_eval=scaler.inverse_transform(v_salt_des_norm)\n",
    "result_eval_des=gscv_result_rf.best_estimator_.predict(df_des_eval)\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.scatter(des_eval,result_eval_des)\n",
    "plt.xlabel(\"Desembalse Guadalteba (hm3)\")\n",
    "plt.ylabel(\"Predicción de la salinidad de la mezcla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicción de la Salinidad_mezcla en función de \"Volumen_desembalse_guadalhorce\"\n",
    "#Importante: Si se va a ejecutar esta celda, previamente se deben seleccionar 40 atributos en el método SelectKBest\n",
    "\n",
    "#Extraemos los valores máximos y mínimos de la variable \"Volumen_desembalse_guadalhorce\" y elegimos 102 elementos para evaluar\n",
    "#el predictor. Estos elementos se normalizan ya que el predictor está optimizado para valores normalizados.\n",
    "max_val_des2=[max(df_dam_x[\"Volumen_desembalse_guadalhorce\"])]\n",
    "max_val_des2=pd.DataFrame(max_val_des2)\n",
    "min_val_des2=[min(df_dam_x[\"Volumen_desembalse_guadalhorce\"])]\n",
    "min_val_des2=pd.DataFrame(min_val_des2)\n",
    "v_salt_des2=np.arange(0.021, 7.240, 0.0725)\n",
    "v_salt_des2=pd.DataFrame(v_salt_des2)\n",
    "v_salt_des2=pd.concat([min_val_des2, v_salt_des2]).reset_index(drop = True) \n",
    "v_salt_des2=v_salt_des2.append(max_val_des2,ignore_index=True)\n",
    "v_salt_des2=v_salt_des2.rename(columns={0:\"Volumen_desembalse_guadalhorce\"})\n",
    "scaler=MinMaxScaler()\n",
    "v_salt_des2_norm=scaler.fit_transform(v_salt_des2)\n",
    "v_salt_des2_norm=pd.DataFrame(v_salt_des2_norm,columns=[\"Volumen_desembalse_guadalhorce\"])\n",
    "\n",
    "\n",
    "v_1=df_dam_feat[\"Caudal_riego\"].median()\n",
    "v_2=df_dam_feat[\"Volumen_riego\"].median()\n",
    "v_3=df_dam_feat[\"Volumen_evaporado_guadalteba\"].median()\n",
    "v_4=df_dam_feat[\"Cota_guadalhorce\"].median()\n",
    "v_5=df_dam_feat[\"Cota_guadalteba\"].median()\n",
    "v_6=df_dam_feat[\"Volumen_embalsado_guadalhorce\"].median()\n",
    "v_7=df_dam_feat[\"Salinidad_guadalhorce\"].median()\n",
    "v_8=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba\"].median()\n",
    "v_9=df_dam_feat[\"Temperatura_Max\"].median()\n",
    "v_10=df_dam_feat[\"Volumen_embalsado_guadalteba\"].median()\n",
    "v_11=df_dam_feat[\"Volumen_total_guadalhorce_guadalteba_conde\"].median()\n",
    "v_12=df_dam_feat[\"Evaporacion\"].median()\n",
    "v_13=df_dam_feat[\"Temperatura_Min\"].median()\n",
    "v_14=df_dam_feat[\"Volumen_total_demandas\"].median()\n",
    "v_15=df_dam_feat[\"Caudal_total_demandas\"].median()\n",
    "v_16=df_dam_feat[\"Total_volumen_abastecido\"].mean()\n",
    "v_17=df_dam_feat[\"Cota_conde\"].median()\n",
    "v_18=df_dam_feat[\"Volumen_desembalse_guadalteba\"].mean()\n",
    "v_19=df_dam_feat[\"Volumen_embalsado_conde\"].median()\n",
    "v_20=df_dam_feat[\"Anyo\"].median()\n",
    "v_21=df_dam_feat[\"Caudal_desembalse_guadalteba\"].median()\n",
    "v_22=df_dam_feat[\"Volumen_abasteciemiento\"].median()\n",
    "v_23=df_dam_feat[\"Variacion_volumen_guadalteba\"].median()\n",
    "v_24=df_dam_feat[\"Caudal_desembalse_guadalhorce\"].median()\n",
    "v_25=df_dam_feat[\"Caudal_abastencimiento\"].median() \n",
    "v_26=df_dam_feat[\"Aportacion_aparente_guadalhorce\"].median()\n",
    "v_27=df_dam_feat[\"Salinidad_conde\"].median()\n",
    "v_28=df_dam_feat[\"Aportacion_aparente_total_guadalhorce_guadalteba\"].median()\n",
    "v_29=df_dam_feat[\"Salinidad_guadalteba\"].median()\n",
    "v_30=df_dam_feat[\"Aportacion_aparente_total_guadalhorce_guadalteba_conde\"].median()\n",
    "v_31=df_dam_feat[\"Variacion_total_guadalhorce_guadalteba\"].median()\n",
    "v_32=df_dam_feat[\"Variacion_total_guadalhorce_guadalteba_conde\"].median()\n",
    "v_33=df_dam_feat[\"Toneladas_sal_guadalteba\"].median()\n",
    "v_34=df_dam_feat[\"Aportacion_aparente_guadalteba\"].median()\n",
    "v_35=df_dam_feat[\"Toneladas_sal_conde\"].median()\n",
    "v_36=df_dam_feat[\"Lluvia\"].median()\n",
    "v_37=df_dam_feat[\"Variacion_volumen_guadalhorce\"].median()\n",
    "v_38=df_dam_feat[\"Variacion_volumen_conde\"].median()\n",
    "v_39=df_dam_feat[\"Aportacion_aparente_conde\"].median()\n",
    "\n",
    "\n",
    "v_1=np.full(shape=102,fill_value=v_1)\n",
    "v_1=pd.DataFrame(v_1)\n",
    "v_2=np.full(shape=102,fill_value=v_2)\n",
    "v_2=pd.DataFrame(v_2)\n",
    "v_3=np.full(shape=102,fill_value=v_3)\n",
    "v_3=pd.DataFrame(v_3)\n",
    "v_4=np.full(shape=102,fill_value=v_4)\n",
    "v_4=pd.DataFrame(v_4)\n",
    "v_5=np.full(shape=102,fill_value=v_5)\n",
    "v_5=pd.DataFrame(v_5)\n",
    "v_6=np.full(shape=102,fill_value=v_6)\n",
    "v_6=pd.DataFrame(v_6)\n",
    "v_7=np.full(shape=102,fill_value=v_7)\n",
    "v_7=pd.DataFrame(v_7)\n",
    "v_8=np.full(shape=102,fill_value=v_8)\n",
    "v_8=pd.DataFrame(v_8)\n",
    "v_9=np.full(shape=102,fill_value=v_9)\n",
    "v_9=pd.DataFrame(v_9)\n",
    "v_10=np.full(shape=102,fill_value=v_10)\n",
    "v_10=pd.DataFrame(v_10)\n",
    "v_11=np.full(shape=102,fill_value=v_11)\n",
    "v_11=pd.DataFrame(v_11)\n",
    "v_12=np.full(shape=102,fill_value=v_12)\n",
    "v_12=pd.DataFrame(v_12)\n",
    "v_13=np.full(shape=102,fill_value=v_13)\n",
    "v_13=pd.DataFrame(v_13)\n",
    "v_14=np.full(shape=102,fill_value=v_14)\n",
    "v_14=pd.DataFrame(v_14)\n",
    "v_15=np.full(shape=102,fill_value=v_15)\n",
    "v_15=pd.DataFrame(v_15)\n",
    "v_16=np.full(shape=102,fill_value=v_16)\n",
    "v_16=pd.DataFrame(v_16)\n",
    "v_17=np.full(shape=102,fill_value=v_17)\n",
    "v_17=pd.DataFrame(v_17)\n",
    "v_18=np.full(shape=102,fill_value=v_18)\n",
    "v_18=pd.DataFrame(v_18)\n",
    "v_19=np.full(shape=102,fill_value=v_19)\n",
    "v_19=pd.DataFrame(v_19)\n",
    "v_20=np.full(shape=102,fill_value=v_20)\n",
    "v_20=pd.DataFrame(v_20)\n",
    "v_21=np.full(shape=102,fill_value=v_21)\n",
    "v_21=pd.DataFrame(v_21)\n",
    "v_22=np.full(shape=102,fill_value=v_22)\n",
    "v_22=pd.DataFrame(v_22)\n",
    "v_23=np.full(shape=102,fill_value=v_23)\n",
    "v_23=pd.DataFrame(v_23)\n",
    "v_24=np.full(shape=102,fill_value=v_24)\n",
    "v_24=pd.DataFrame(v_24)\n",
    "v_25=np.full(shape=102,fill_value=v_25)\n",
    "v_25=pd.DataFrame(v_25)\n",
    "v_26=np.full(shape=102,fill_value=v_26)\n",
    "v_26=pd.DataFrame(v_26)\n",
    "v_27=np.full(shape=102,fill_value=v_27)\n",
    "v_27=pd.DataFrame(v_27)\n",
    "v_28=np.full(shape=102,fill_value=v_28)\n",
    "v_28=pd.DataFrame(v_28)\n",
    "v_29=np.full(shape=102,fill_value=v_29)\n",
    "v_29=pd.DataFrame(v_29)\n",
    "v_30=np.full(shape=102,fill_value=v_30)\n",
    "v_30=pd.DataFrame(v_30)\n",
    "v_31=np.full(shape=102,fill_value=v_31)\n",
    "v_31=pd.DataFrame(v_31)\n",
    "v_32=np.full(shape=102,fill_value=v_32)\n",
    "v_32=pd.DataFrame(v_32)\n",
    "v_33=np.full(shape=102,fill_value=v_33)\n",
    "v_33=pd.DataFrame(v_33)\n",
    "v_34=np.full(shape=102,fill_value=v_34)\n",
    "v_34=pd.DataFrame(v_34)\n",
    "v_35=np.full(shape=102,fill_value=v_35)\n",
    "v_35=pd.DataFrame(v_35)\n",
    "v_36=np.full(shape=102,fill_value=v_36)\n",
    "v_36=pd.DataFrame(v_36)\n",
    "v_37=np.full(shape=102,fill_value=v_37)\n",
    "v_37=pd.DataFrame(v_37)\n",
    "v_38=np.full(shape=102,fill_value=v_38)\n",
    "v_38=pd.DataFrame(v_38)\n",
    "v_39=np.full(shape=102,fill_value=v_39)\n",
    "v_39=pd.DataFrame(v_39)\n",
    "\n",
    "\n",
    "column_names3=[\"Caudal_riego\",\"Volumen_riego\",\"Volumen_evaporado_guadalteba\",\"Cota_guadalhorce\",\"Cota_guadalteba\",\n",
    "             \"Volumen_embalsado_guadalhorce\",\"Salinidad_guadalhorce\",\"Volumen_total_guadalhorce_guadalteba\",\n",
    "             \"Temperatura_Max\",\"Volumen_embalsado_guadalteba\",\"Volumen_total_guadalhorce_guadalteba_conde\",\n",
    "             \"Evaporacion\",\"Temperatura_Min\",\"Volumen_total_demandas\",\"Caudal_total_demandas\",\n",
    "             \"Total_volumen_abastecido\",\"Cota_conde\",\"Volumen_desembalse_guadalteba\",\"Volumen_embalsado_conde\",\n",
    "             \"Anyo\",\"Caudal_desembalse_guadalteba\",\"Volumen_abasteciemiento\",\"Variacion_volumen_guadalteba\",\"Caudal_desembalse_guadalhorce\",\n",
    "\"Caudal_abastencimiento\",\"Aportacion_aparente_guadalhorce\",\"Salinidad_conde\",\"Aportacion_aparente_total_guadalhorce_guadalteba\",\n",
    "\"Salinidad_guadalteba\",\"Aportacion_aparente_total_guadalhorce_guadalteba_conde\",\"Variacion_total_guadalhorce_guadalteba\",\n",
    "\"Variacion_total_guadalhorce_guadalteba_conde\",\"Toneladas_sal_guadalteba\",\"Volumen_desembalse_guadalhorce\",\n",
    "\"Aportacion_aparente_guadalteba\",\"Toneladas_sal_conde\",\"Lluvia\",\"Variacion_volumen_guadalhorce\",\"Variacion_volumen_conde\",\n",
    "\"Aportacion_aparente_conde\"]\n",
    "\n",
    "df_des2_eval=pd.concat([v_1,v_2,v_3,v_10,v_5,v_6,v_4,v_7,v_8,v_9,v_17,v_11,v_12,v_13,v_14,v_15,v_16,v_18,v_19,\n",
    "                       v_20,v_21,v_22,v_23,v_24,v_25,v_26,v_27,v_28,v_29,v_30,v_31,v_32,v_33,v_salt_des2,v_34,\n",
    "                        v_35,v_36,v_37,v_38,v_39],axis=1)\n",
    "df_des2_eval.columns=column_names3\n",
    "\n",
    "\n",
    "#Usamos el mejor predictor sobre el dataframe creado\n",
    "\n",
    "#Deshacemos la normalización en la variable \"Volumen_desembalse_guadalhorce\" para ver la clasificación de \"Salinidad_mezcla\"\n",
    "#en función de los valores reales de \"Volumen_desembalse_guadalhorce\"\n",
    "des2_eval=scaler.inverse_transform(v_salt_des2_norm)\n",
    "result_eval_des2=gscv_result_rf.best_estimator_.predict(df_des2_eval)\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.scatter(des2_eval,result_eval_des2)\n",
    "plt.xlabel(\"Desembalse Guadalhorce (hm3)\")\n",
    "plt.ylabel(\"Predicción de la salinidad de la mezcla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 Distribución de variable etiquetada trás métodos de imputación de NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con mean()\n",
    "plt_df=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_mean':df_data['Salinidad_mezcla'].fillna((df_data['Salinidad_mezcla'].mean()))})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df[['var_mean']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación mean()'])\n",
    "print(plt_df['var'].describe())\n",
    "print(plt_df['var_mean'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con median()\n",
    "plt_df=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_median':df_data['Salinidad_mezcla'].fillna((df_data['Salinidad_mezcla'].median()))})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df[['var_median']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación median()'])\n",
    "print(plt_df['var'].describe())\n",
    "print(plt_df['var_median'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con rolling mean()\n",
    "window=30\n",
    "plt_df_roll_mean=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_roll_mean':df_data['Salinidad_mezcla'].fillna((df_data['Salinidad_mezcla'].rolling(window,min_periods=1,).mean()))})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df_roll_mean[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df_roll_mean[['var_roll_mean']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación rolling mean()'])\n",
    "print(plt_df_roll_mean['var'].describe())\n",
    "print(plt_df_roll_mean['var_roll_mean'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con rolling median()\n",
    "window=30\n",
    "plt_df_roll_median=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_roll_median':df_data['Salinidad_mezcla'].fillna((df_data['Salinidad_mezcla'].rolling(window,min_periods=1,).median()))})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df_roll_median[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df_roll_median[['var_roll_median']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación rolling median()'])\n",
    "print(plt_df_roll_median['var'].describe())\n",
    "print(plt_df_roll_median['var_roll_median'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con interpolación lineal\n",
    "plt_df_linear_inter=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_linear_inter':df_data.Salinidad_mezcla.interpolate(method='linear')})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df_linear_inter[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df_linear_inter[['var_linear_inter']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación interpolación lineal'])\n",
    "print(plt_df_linear_inter['var'].describe())\n",
    "print(plt_df_linear_inter['var_linear_inter'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con interpolación cuadrática\n",
    "plt_df_linear_quad=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_linear_quad':df_data.Salinidad_mezcla.interpolate(method='quadratic')})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df_linear_quad[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df_linear_quad[['var_linear_quad']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación interpolación cuadrática'])\n",
    "print(plt_df_linear_quad['var'].describe())\n",
    "print(plt_df_linear_quad['var_linear_quad'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con interpolación cúbica\n",
    "plt_df_linear_cubic=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_linear_cubic':df_data.Salinidad_mezcla.interpolate(method='cubic')})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df_linear_cubic[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df_linear_cubic[['var_linear_cubic']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación interpolación cúbica'])\n",
    "print(plt_df_linear_cubic['var'].describe())\n",
    "print(plt_df_linear_cubic['var_linear_cubic'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con interpolación akima\n",
    "plt_df_linear_akima=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_linear_akima':df_data.Salinidad_mezcla.interpolate(method='akima')})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df_linear_akima[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df_linear_akima[['var_linear_akima']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación interpolación akima'])\n",
    "print(plt_df_linear_akima['var'].describe())\n",
    "print(plt_df_linear_akima['var_linear_akima'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución de los valores en la variable Salinidad_mezcla con NANs y la variable Salinidad_mezcla con imputación \n",
    "#con interpolación spline\n",
    "plt_df_linear_spline=pd.DataFrame({'var':df_data['Salinidad_mezcla'],'var_linear_spline':df_data.Salinidad_mezcla.interpolate(method='spline',order=3)})\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "sns.distplot(plt_df_linear_spline[['var']], hist=False,label=\"Variable Salinidad_mezcla sin preprocesar\",kde_kws={\"shade\": True})\n",
    "sns.distplot(plt_df_linear_spline[['var_linear_spline']],axlabel=\"Salinidad_mezcla\", hist=False, kde_kws={\"shade\": True})\n",
    "fig.legend(labels=['Salinidad_mezcla sin preprocesar','Salinidad_mezcla con imputación interpolación spline'])\n",
    "print(plt_df_linear_spline['var'].describe())\n",
    "print(plt_df_linear_spline['var_linear_spline'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REFERENCIAS\n",
    "\n",
    "#1. https://stackoverflow.com/questions/58072683/combine-year-month-and-day-in-python-to-create-a-date\n",
    "#2. https://stackoverflow.com/questions/52291519/pandas-cant-convert-datetime-yyyy-mm-dd-to-dd-mm-yyyy\n",
    "#3. https://stackoverflow.com/questions/25122099/move-column-by-name-to-front-of-table-in-pandas\n",
    "#4. https://matthew-brett.github.io/teaching/string_formatting.html\n",
    "#5. https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "#6. https://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s\n",
    "#7. https://www.delftstack.com/howto/python-pandas/how-to-delete-dataframe-row-in-pandas-based-on-column-value/\n",
    "#8. https://stackoverflow.com/questions/37724225/how-to-categorize-floating-values-in-python-using-pandas-library\n",
    "#9. https://www.youtube.com/watch?v=mnKm3YP56PY\n",
    "#10.https://www.researchgate.net/publication/224196395_Feature_Selection_Using_Principal_Component_Analysis\n",
    "#11.https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "#12.https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "#13.https://medium.com/@nmscott14/3-feature-selection-methods-e7ccd6dbf316\n",
    "#14.https://towardsdatascience.com/predict-missing-values-in-the-dataset-897912a54b7b\n",
    "#15.https://towardsdatascience.com/why-and-how-to-cross-validate-a-model-d6424b45261f\n",
    "#16.https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "#17.https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
