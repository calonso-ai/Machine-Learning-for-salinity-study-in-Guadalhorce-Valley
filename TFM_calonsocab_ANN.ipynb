{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM UOC SALINIDAD EN EL VALLE DEL GUADALHORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan las librerias que serán necesarias durante todo el desarrollo del script\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from datetime import datetime\n",
    "#import pydotplus\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "#import pydotplus\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#import missingno as msno\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from numpy import random\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga e integración de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso que se deberán cargar e integrar los datos, ya que se encuentran dispersos en distintos ficheros.\n",
    "\n",
    "A continuación se ejecuta un código que va recorriendo cada fichero CSV y va extrayendo la información de cada variable. Se crea una variable fecha, con el día, mes y año donde se ha tomada cada dato. \n",
    "\n",
    "Por lo tanto, crearemos un dataframe con todos los datos recogidos por la Junta de Andalucía en la explotación del sistema de presas del valle del guadalhorce. \n",
    "\n",
    "Adicionalmente se visualiza el dataset completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se leen los ficheros csv correspondientes.\n",
    "\n",
    "df_list=[]\n",
    "for anyo in range(12,20):\n",
    "    path_csv=f'dataset/ESTADILLO_DE_EMBALSES_20{anyo}.csv'\n",
    "    df=pd.read_csv(path_csv,sep=\";\",encoding = 'unicode_escape',decimal=',',thousands='.')\n",
    "    #Se consolidan las variables Dia, Mes y Anyo en una variable de nueva creación llamada Fecha\n",
    "    df['aux']=df['Anyo'].astype(str) + df['Mes'].astype(str).str.zfill(2)+ df['Dia'].astype(str).str.zfill(2)\n",
    "    df['Fecha'] = pd.to_datetime(df['aux'], format='%Y%m%d')\n",
    "    #Eliminamos la columna auxilir 'aux'\n",
    "    del df['aux']\n",
    "    #Se cambia el formato de YYYY-MM-DD a DD-MM-YYYY\n",
    "    df['Fecha'] = df['Fecha'].dt.strftime('%d-%m-%Y')\n",
    "    #Movemos el campo fecha al comienzo del dataframe\n",
    "    indice=df['Fecha']\n",
    "    df.drop(labels=['Fecha'], axis=1,inplace = True)\n",
    "    df.insert(0, 'Fecha', indice)\n",
    "    if anyo==14:\n",
    "        df.drop(df.columns[[74,75,76,77,78,79,80,81,82,83,84,85,86]],axis = 1, inplace = True)\n",
    "    if anyo==15:\n",
    "        df.drop(df.columns[[74,75,76,77,78,79,80,81,82,83,84,85,86]],axis = 1, inplace = True)\n",
    "    df_list.append(df)\n",
    "\n",
    "df_data=pd.concat(df_list,ignore_index = True)\n",
    "#Se consolidan todos los dataframes de la lista en un solo dataframe. Así, quedará un dataframe que contenga los\n",
    "#datos díarios dentro de un año.\n",
    "    \n",
    "#Se utiliza para visualizar todas las filas del dataframe creado\n",
    "pd.set_option('display.max_rows', df_data.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se visualiza el dataset completo\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpieza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se detectan todos los missing values existentes en el dataset y se aplican distintas estrategias de imputación de la variable etiquetada.\n",
    "\n",
    "Se realiza una imputación de NAN de la variable Salinidad_guadalhorce usando regresion logística.\n",
    "\n",
    "También se limpia el dataset de las columnas vacias, carácteres inadecuados y se cambia el separador decimal de \",\" a \".\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podemos visutalizar la cantidad de missing values presentes en el dataset (iterar para ver todas las columnas)\n",
    "msno.matrix(df_data.iloc[:,25:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porcentaje de NAN en la variable etiquetada\n",
    "salinidad_mezcla_percentage = 100*df_data['Salinidad_mezcla'].isnull().sum()/len(df_data['Salinidad_mezcla'])\n",
    "salinidad_mezcla_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las columnas con todos los valores a 0\n",
    "df_data=df_data.drop(df.columns[25], axis=1)\n",
    "df_data=df_data.drop(df.columns[71], axis=1)\n",
    "df_data=df_data.drop(df.columns[72], axis=1)\n",
    "df_data=df_data.drop(df.columns[73], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea la variable df_dam que contiene todos los datos de la explotación de las presas del valle del guadalhorce\n",
    "df_dam=df_data\n",
    "#Se elimina un valor que está dando error\n",
    "indexNames = df_dam[df_dam['Salinidad_mezcla'] == '1,170' ].index\n",
    "df_dam.drop(indexNames , inplace=True)\n",
    "\n",
    "#Ajustamos el tipo de la variable Salinidad_mezcla para poder procesarla\n",
    "df_dam['Salinidad_mezcla']=df_dam['Salinidad_mezcla'].astype(str).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################-------------------------IMPUTACIÓN NAN EN SALINIDAD MEZCLA------------------###################\n",
    "\n",
    "#Activar solamente un tipo de imputación.\n",
    "\n",
    "#1) BORRAR LAS FILAS QUE CONTIENEN NAN\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "\n",
    "#2) REGRESOR LINEAL PARA PREDECIR LOS VALORES DE NAN\n",
    "#cols_sm=[\"Cota_guadalhorce\",\"Cota_guadalteba\",\"Volumen_total_guadalhorce_guadalteba_conde\",\"Desembalse_total_guadalhorce_guadalteba_conde\"\n",
    "      #,\"Salinidad_mezcla\"]\n",
    "\n",
    "#df_sm=df_dam[cols_sm]\n",
    "#test_df_sm=df_sm[df_sm[\"Salinidad_mezcla\"].isnull()]\n",
    "#df_sm=df_sm.dropna()\n",
    "\n",
    "#ynan_train_sm=df_sm[\"Salinidad_mezcla\"]\n",
    "#Xnan_train_sm=df_sm.drop(\"Salinidad_mezcla\",axis=1)\n",
    "#Xnan_test_sm=test_df_sm.drop(\"Salinidad_mezcla\",axis=1)\n",
    "\n",
    "#Recurrimos a la regresión logistica para predecir el valor de los NAN.\n",
    "#lr_sm=LinearRegression()\n",
    "#lr_sm.fit(Xnan_train_sm, ynan_train_sm)\n",
    "#ynan_pred_sm=lr_sm.predict(Xnan_test_sm)\n",
    "\n",
    "#Se sustituyen los valores NAN por sus predicciones\n",
    "#df_dam.loc[df_dam.Salinidad_mezcla.isnull(),'Salinidad_mezcla']=ynan_pred_sm\n",
    "\n",
    "#3) IMPUTACIÓN CON MEDIA\n",
    "#df_dam['Salinidad_mezcla'].fillna((df_dam['Salinidad_mezcla'].mean()),inplace=True)\n",
    "\n",
    "#4) IMPUTACIÓN CON MEDIANA (con este obtenemos mas que con la predicción)\n",
    "#df_dam['Salinidad_mezcla'].fillna((df_dam['Salinidad_mezcla'].median()),inplace=True)\n",
    "\n",
    "#5) IMPUTACIÓN CON ROLLING MEAN (y eliminación de los NANs restantes)\n",
    "#window = 30 # 30 dias de observación\n",
    "#df_dam['Salinidad_mezcla'].fillna((df_dam['Salinidad_mezcla'].rolling(window,min_periods=1,).mean()),inplace=True)\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#6) IMPUTACIÓN CON ROLLING MEDIAN\n",
    "#window = 30 # 30 dias de observación\n",
    "#df_dam['Salinidad_mezcla'].fillna((df_dam['Salinidad_mezcla'].rolling(window,min_periods=1,).median()),inplace=True)\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#7) IMPUTACIÓN CON INTERPOLACIÓN LINEAL\n",
    "df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='linear')\n",
    "\n",
    "#8) IMPUTACIÓN CON INTERPOLACIÓN CUADRATICA\n",
    "#df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='quadratic')\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#9) IMPUTACIÓN CON INTERPOLACIÓN CUBICA\n",
    "#df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='cubic')\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#10) IMPUTACIÓN CON INTERPOLACIÓN AKIMA\n",
    "#df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='akima')\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)\n",
    "\n",
    "#11) IMPUTACIÓN CON INTERPOLACIÓN SPLINE\n",
    "#df_dam[\"Salinidad_mezcla\"]=df_dam.Salinidad_mezcla.interpolate(method='spline',order=3)\n",
    "#df_dam.dropna(subset=['Salinidad_mezcla'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se va usa unar regeresion logistica para calucular el valor de los NAN de la variable \"Salinidad_guadalhorce\"\n",
    "#Planteamos dos escenarios en la variable \"Salinidad_mezcla\", quitando los NAN y sustituyendo por la media\n",
    "#Implementamos por un lado KFold cross-validation y, posteriormente GridSearchCV\n",
    "\n",
    "#Se va usa unar regeresion logistica para calucular el valor de los NAN de la variable \"Salinidad_guadalhorce\"\n",
    "\n",
    "#Usamos un regresor lineal para predecir los valores NAN de la varibale Salinidad_guadalhorce.\n",
    "cols=[\"Cota_guadalhorce\",\"Salinidad_mezcla\",\"Salinidad_guadalhorce\"]\n",
    "\n",
    "#Se pueden meter mas variables como \"Variacion_volumen_guadalhorce\", \"Volumen_evaporado_guadalhorce\" (esta ultima\n",
    "#habria que quitarle los puntos y pasarla a float antes de meterla en el dataset de predicción)\n",
    "df=df_dam[cols]\n",
    "test_df=df[df[\"Salinidad_guadalhorce\"].isnull()]\n",
    "df=df.dropna()\n",
    "\n",
    "ynan_train=df[\"Salinidad_guadalhorce\"]\n",
    "Xnan_train=df.drop(\"Salinidad_guadalhorce\",axis=1)\n",
    "Xnan_test=test_df.drop(\"Salinidad_guadalhorce\",axis=1)\n",
    "\n",
    "#Recurrimos a la regresión logistica para predecir el valor de los NAN.\n",
    "lr=LinearRegression()\n",
    "lr.fit(Xnan_train, ynan_train)\n",
    "ynan_pred=lr.predict(Xnan_test)\n",
    "\n",
    "#Se sustituyen los valores NAN por sus predicciones\n",
    "df_dam.loc[df_dam.Salinidad_guadalhorce.isnull(),'Salinidad_guadalhorce']=ynan_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porcentaje de NAN trás la imputación\n",
    "salinidad_mezcla_na_percentage = 100*df_dam['Salinidad_mezcla'].isnull().sum()/len(df_dam['Salinidad_mezcla'])\n",
    "salinidad_mezcla_na_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Discretización de la variable etiquetada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La discretización de datos es el proceso de establecer varios puntos de corte para atributos con valores numéricos continuos con el fin de obtener valores enteros o discretos de dichos atributos.\n",
    "\n",
    "En este caso, se ha contado con la Junta de Andalucía para realizar la asignación de intervalos:\n",
    "\n",
    "-\tMezcla de aguas no salina: 0 ≤  Salinidad_mezcla ≤ 900: Salinidad_mezcla = 0 \n",
    "\n",
    "-\tMezcla de aguas salina: Salinidad_mezcla > 900: Salinidad_mezcla = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discretiza la variable Salinidad_mezcla en 1 y 0. 0 para todos los valores por debajo de 900 y 1 para todos los\n",
    "#valores por encima\n",
    "\n",
    "#En este caso se ha subido a 900. \n",
    "bins=[0,900,5000]\n",
    "df_dam['Salinidad_mezcla_tag'] = pd.cut(df_dam['Salinidad_mezcla'], bins, labels=[0,1])\n",
    "\n",
    "#Sustituimos todos los valores NaN del df por 0\n",
    "df_dam=df_dam.fillna(0)\n",
    "#Borra la columna Salinidad_mezcla después de categorizarla\n",
    "df_dam=df_dam.drop(['Salinidad_mezcla'],axis=1)\n",
    "\n",
    "#Separamos la variable etiquetada del resto de dataset\n",
    "df_dam_y=df_dam['Salinidad_mezcla_tag']\n",
    "df_dam_x=df_dam.drop(df_dam[['Salinidad_mezcla_tag','Fecha','Hora','Horas_abastecimiento','Horas_ecologico',\n",
    "                       'Horas_riego','Tiempo_desembalse_guadalhorce','Tiempo_desembalse_guadalteba',\n",
    "                        'Volumen_central-reversible_tajo',\n",
    "                        'Variacion_total_gaitanejo_tajo','Toneladas_sal_guadalhorce',\n",
    "                        'Agua_sobrante','Toneladas_sal_mezcla']],axis=1)\n",
    "\n",
    "\n",
    "#Eliminamos las comas que han quedado y algunos caracteres especiales\n",
    "for i in range(len(df_dam_x.columns)):\n",
    "    col = df_dam_x.columns[i]\n",
    "    if df_dam_x[col].dtypes==object:\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace(',','.')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('#ÁREF!','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('V','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('VAR','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('var','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('0AR','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('12;0','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('9:00','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('-0.073','0')\n",
    "        df_dam_x[col] = df_dam_x[col].str.replace('1.6-1.5','0')\n",
    "\n",
    "#Convertimos algunas variables de \"object\" a \"float\"        \n",
    "df_dam_x['Caudal_desembalse_guadalhorce']=df_dam_x['Caudal_desembalse_guadalhorce'].astype(float)\n",
    "df_dam_x['Volumen_evaporado_guadalhorce']=df_dam_x['Volumen_evaporado_guadalhorce'].astype(float)\n",
    "df_dam_x['Volumen_total_gaitanejo_tajo']=df_dam_x['Volumen_total_gaitanejo_tajo'].astype(float)\n",
    "df_dam_x['Caudal_desembalse_guadalteba']=df_dam_x['Caudal_desembalse_guadalteba'].astype(float)\n",
    "df_dam_x['Caudal_abastencimiento']=df_dam_x['Caudal_abastencimiento'].astype(float)\n",
    "df_dam_x['Lluvia']=df_dam_x['Lluvia'].astype(float)        \n",
    "\n",
    "#Convertimos los NAN a 0 de algunas variables, trás la conversión a float\n",
    "df_dam_x['Caudal_desembalse_guadalhorce']=df_dam_x['Caudal_desembalse_guadalhorce'].fillna(0)\n",
    "df_dam_x['Volumen_evaporado_guadalhorce']=df_dam_x['Volumen_evaporado_guadalhorce'].fillna(0)\n",
    "df_dam_x['Volumen_total_gaitanejo_tajo']=df_dam_x['Volumen_total_gaitanejo_tajo'].fillna(0)\n",
    "df_dam_x['Caudal_desembalse_guadalteba']=df_dam_x['Caudal_desembalse_guadalteba'].fillna(0)\n",
    "df_dam_x['Caudal_abastencimiento']=df_dam_x['Caudal_abastencimiento'].fillna(0)\n",
    "df_dam_x['Lluvia']=df_dam_x['Lluvia'].fillna(0)\n",
    "\n",
    "#Vamos a normalizar todas las variables de df_dam_x usando MinMaxScaler.\n",
    "scaler=MinMaxScaler()\n",
    "norm_df=scaler.fit_transform(df_dam_x)\n",
    "\n",
    "#Se vuelve crear un dataframe ya que al normalizar se realiza una conversión a numpy.ndarray\n",
    "norm_df_dam_x=pd.DataFrame(norm_df,columns=list(df_dam_x.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de ver cuales son las variables que mas influyen en la salinidad de la mezcla vamos a obtener los 20 atributos mas relevantes a través de SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features selection con SelectKBest y ANOVA\n",
    "\n",
    "#Extraemos los nombres de las variables\n",
    "features_list=list(norm_df_dam_x.columns)\n",
    "\n",
    "#Aplicación de SelectKBest con el método ANOVA para seleccionar 20 variables mas influyentes\n",
    "features_test = SelectKBest(score_func=f_classif, k=20)\n",
    "fit = features_test.fit(norm_df_dam_x, df_dam_y)\n",
    "\n",
    "set_printoptions(precision=4)\n",
    "print(fit.scores_)\n",
    "#Los valores nan corresponden a las variables \"Filtraciones_guadalhorce\",\"Filtraciones_guadalteba\" y \"Min_ecologico\".\n",
    "#Resultan nan porque son constantes en el caso de las dos primeras, y en el caso de Min_ecologico, todos los valores\n",
    "#son 0, aunque al incio no se detecten como tal.\n",
    "features = fit.transform(norm_df_dam_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se imprimen las 20 variables mas relevantes\n",
    "df_scores=pd.DataFrame(fit.scores_)\n",
    "df_columns=pd.DataFrame(features_list)\n",
    "\n",
    "feature_scores = pd.concat([df_columns, df_scores],axis=1)\n",
    "feature_scores.columns = ['Feature_Name','Score']  \n",
    "print(feature_scores.nlargest(20,'Score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Aplicación de métodos de ML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado el conjunto de datos resultante de la fase de preprocesado, el siguiente paso que se pretende resolver, con la aplicación de los distintos métodos de aprendizaje automático, un problema de clasificación binaria de la variable Salinidad_mezcla. En este tipo de problemas se pretende encontrar un predictor o clasificador con el fin de predecir la clase de la variable etiquetada, en este caso, predecir la clase 0, correspondiente con muestras de agua no salina, o la clase 1, correspondiente con muestras de agua salina "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se separan de forma aleatoria los conjuntos de entrenamiento y test según la regla 85%-15%.\n",
    "X_train_gs, X_test_gs, y_train_gs, y_test_gs = train_test_split(df_dam_feat, df_dam_y, test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta celda es usada para visualizar las fronteras de decisión.\n",
    "#Se separan los conjuntos considerando únicamente las variables \"Salinidad_guadalhorce\" y \"Cota_guadalhorce\"\n",
    "#Aqui no se usa cross-validation por lo tanto test set es de 30%\n",
    "bound_df=df_dam_feat[['Cota_guadalhorce','Salinidad_guadalhorce']]\n",
    "X_train_bound, X_test_bound, y_train_bound, y_test_bound = train_test_split(bound_df, df_dam_y, \n",
    "                                                                            test_size=0.10,random_state=42)\n",
    "\n",
    "#Se crea la siguiente función para visualizar con los distintos algoritomos las fronteras de decisión\n",
    "x_min, x_max = df_dam_feat['Cota_guadalhorce'].min()-0.1, df_dam_feat['Cota_guadalhorce'].max()+0.1\n",
    "y_min, y_max = df_dam_feat['Salinidad_guadalhorce'].min()-0.1, df_dam_feat['Salinidad_guadalhorce'].max()+0.1  \n",
    "\n",
    "def plot_decision_boundaries(x, y, labels, model, \n",
    "                             x_min=x_min, \n",
    "                             x_max=x_max, \n",
    "                             y_min=y_min, \n",
    "                             y_max=y_max, \n",
    "                             grid_step=0.02):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n",
    "                         np.arange(y_min, y_max, grid_step))\n",
    "    \n",
    "    # Predecimos el classifier con los valores de la meshgrid.\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "\n",
    "    # Hacemos reshape para tener el formato correcto.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Seleccionamos una paleta de color.\n",
    "    arr = plt.cm.coolwarm(np.arange(plt.cm.coolwarm.N))\n",
    "    arr_hsv = mpl.colors.rgb_to_hsv(arr[:,0:3])\n",
    "    arr_hsv[:,2] = arr_hsv[:,2] * 1.5\n",
    "    arr_hsv[:,1] = arr_hsv[:,1] * .5\n",
    "    arr_hsv = np.clip(arr_hsv, 0, 1)\n",
    "    arr[:,0:3] = mpl.colors.hsv_to_rgb(arr_hsv) \n",
    "    my_cmap = ListedColormap(arr)\n",
    "    \n",
    "    # Hacemos el gráfico de las fronteras de decisión.\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=my_cmap)\n",
    "\n",
    "    # Añadimos los punts.\n",
    "    ax.scatter(x, y, c=labels, cmap='coolwarm')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recurrimos a la función definida en el ejercicio anterior, con la particularidad de que añadiremos una capa oculta\n",
    "#a nuestra red neuronal y le pasaremos el número de neuronas que contendrá dicha capa oculta\n",
    "def create_model_ann(learn_rate,units):\n",
    "    # Construimoso el modelo a través de Keras. A partir de un modelo secuencial creado con Sequential, \n",
    "    #añadimos capas con Dense.\n",
    "    model=Sequential()\n",
    "    #Añadimos la capa de salida, indicandole que la dimensión de la entrada es 2 y que la función de activación es \n",
    "    #softmax. Se le indica que no se desea vector de bias.\n",
    "    model.add(Dense(units, input_dim=20, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax',use_bias=False,name=\"Output_layer\"))\n",
    "    #Se realiza un resumen del modelo obtenido.\n",
    "    model.summary()\n",
    "    # #Una vez que el modelo está definido, compilamos. A la hora de compilar le indicamos el optimizador a usar, que \n",
    "    #en este caso será SGD. Además se le indica que la métrica a tener en cuenta es la precisión, como en los ejemplos\n",
    "    #anteriores. El atributo 'loss' se configura como 'sparse_categorical_crossentropy' para evitar el one-hot-enconding\n",
    "    optimizer = SGD(lr=learn_rate)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se construye el vector de hiperparámetros elegidos aleatoriamente en el rango indicado\n",
    "n_epoch=[50,75,100]\n",
    "# n_epoch=[50]\n",
    "lr=[0.1,0.5,0.9]\n",
    "#lr=[0.1]\n",
    "units=[10,30,40]\n",
    "#units=[10]\n",
    "param_grid_ann = dict(epochs=n_epoch,learn_rate=lr,units=units)\n",
    "\n",
    "#Se crea el modelo a través de la función create_model2 definida mas arriba. Con el parámetro n_iter=20 le indicamos a la función que se\n",
    "#usarán 20 combinaciones de hiperparámetros elegidos aleatoriamente.\n",
    "model_ann=KerasClassifier(build_fn=create_model_ann)\n",
    "#gscv_ann=GridSearchCV(model_ann,param_grid_ann, cv=10, n_jobs=-1,scoring='accuracy')\n",
    "gscv_ann=GridSearchCV(model_ann,param_grid_ann, cv=10, n_jobs=1,scoring='accuracy',verbose=1)\n",
    "#Se usa el conjunto de datos original (sin one-hot-encoded) ya que el atributo 'loss' se configuró como\n",
    "#sparse_categorical_crossentropy, considerando a 'y' como atributo categorico.\n",
    "gscv_result_ann=gscv_ann.fit(X_train_gs,y_train_gs)\n",
    "\n",
    "#Una vez se evaluan el modelo de neural network para los hiperparámetros se obtienen la mejor precisión del modelo \n",
    "#y los hiperparámetros a partir de los cuales conseguimos dicho ratio.\n",
    "print(gscv_result_ann.best_score_)\n",
    "print(gscv_result_ann.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construimos el dataframe con el que se contruirá el heatmap\n",
    "sc6b=gscv_result_ann.cv_results_['mean_test_score']\n",
    "param6b=gscv_result_ann.cv_results_['params']\n",
    "print(param6b)\n",
    "gscv_result_ann.cv_results_\n",
    "aux1_6b=pd.DataFrame(param6b)\n",
    "aux2_6b=pd.DataFrame({'Accuracy':sc6b})\n",
    "df_rnd6b=pd.concat([aux1_6b,aux2_6b],axis=1)\n",
    "df_rnd6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos el poder predictor del clasificador hallado (best_estimator_)\n",
    "y_pred_ann = gscv_result_ann.best_estimator_.predict(X_test_gs)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_gs, y_pred_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REFERENCIAS\n",
    "\n",
    "#1. https://stackoverflow.com/questions/58072683/combine-year-month-and-day-in-python-to-create-a-date\n",
    "#2. https://stackoverflow.com/questions/52291519/pandas-cant-convert-datetime-yyyy-mm-dd-to-dd-mm-yyyy\n",
    "#3. https://stackoverflow.com/questions/25122099/move-column-by-name-to-front-of-table-in-pandas\n",
    "#4. https://matthew-brett.github.io/teaching/string_formatting.html\n",
    "#5. https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "#6. https://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s\n",
    "#7. https://www.delftstack.com/howto/python-pandas/how-to-delete-dataframe-row-in-pandas-based-on-column-value/\n",
    "#8. https://stackoverflow.com/questions/37724225/how-to-categorize-floating-values-in-python-using-pandas-library\n",
    "#9. https://www.youtube.com/watch?v=mnKm3YP56PY\n",
    "#10.https://www.researchgate.net/publication/224196395_Feature_Selection_Using_Principal_Component_Analysis\n",
    "#11.https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "#12.https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "#13.https://medium.com/@nmscott14/3-feature-selection-methods-e7ccd6dbf316\n",
    "#14.https://towardsdatascience.com/predict-missing-values-in-the-dataset-897912a54b7b\n",
    "#15.https://towardsdatascience.com/why-and-how-to-cross-validate-a-model-d6424b45261f\n",
    "#16.https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "#17.https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
